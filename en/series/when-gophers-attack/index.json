[{"content":"If you\u0026rsquo;ve been following my earlier articles you probably know that the previous version of the site was scaffolded from zim notebooks (in this particular case, from a fork made by me including modifications on the code highlithging plugin, and then a manual RSS generator from parsed notebooks)\nAlso, if you\u0026rsquo;re looking at this article you may have noticed I migrated from that modus operandi to a hugo static site.\nRationale The reasons are the following (in descending order):\n Mobile support: The older web template didn\u0026rsquo;t have proper mobile support, and I didn\u0026rsquo;t have the muscle to rewrite the template and the style to make it work Easier maintenance: Having to work on a personal fork on a tool for a purpose that is not the intended one (altough the HTML export feature of zim is there other scenarios) is a workflow prone to errors and maintenance burden Developer support: Hugo is one of the most popular website generators as of today and aside from having an extended community it also counts with a large variety of templates to choose from, and a standard basis upon which all of these generate the web content (frontmatter + markdown)  Some howto(s) Hugo\u0026rsquo;s website has a quickstart tutorial. The main ideas being\n1 - You set up an hugo configuration file\n2 - You have to choose a theme, recommended as a github submodule (unless you want to do some heavy customization), but it is a part of your site\u0026rsquo;s directory structure (since the content will be generated from there)\n3 - Write your articles. Format is standard markdown (and others), with a frontmatter header. Article location is theme dependant\n4 - Preview your content locally with hugo serve\n5 - Deploy your site (to be continued\u0026hellip;)\nIn my case, the theme I ended up choosing is zzo, which provides a lot a features and a really nice user interface to leverage them\nOther tasks done but that won\u0026rsquo;t be detailed here  Rewriting all RSS entry URLs Rewriting all zim entries as markdown hugo posts. Part of this was done with a modified version of the RSS scaffolder Rewriting all resource URLs to match their new locations Using a logo and reimplementing favicons Further CSS template customization (TODO)  Deploying the site As told in the official guide, hugo generates the static HTML content based on the markdown posts, the theme and the configurations.\nFor the case of a github pages repo, a fully working site with minified assets will be generated with no further effort with the following command:\nhugo -D --minify -d ./ ","description":"The hard but rewarding road of website migration to Hugo","id":0,"section":"","tags":["Golang","Hugo","Migration"],"title":"Migrating the site to Hugo","uri":"https://luisgg.me/en/migrating_the_site_to_hugo/"},{"content":"The RSS feed for this site is available at https://luisgg.me/rss.xml\nSince this site operates in a wiki structure and format, the RSS is the chronological index of the site entries by default, and thus the best way of seeing them in an ordered way. Another plus is the advantages the RSS readers may offer such as subscription for notifications on new entries.\n","description":"","id":1,"section":"","tags":["RSS"],"title":"About RSS feed","uri":"https://luisgg.me/en/about_rss_feed/"},{"content":"The story I have to say, this is (and I just realized a couple of weeks ago) my longest lived project without being deprecated/abandoned. By the time of this publication, It\u0026rsquo;s been already 3 years in the making. Why? well, mostly because of 3 things\n It went though several reimplementations It is a part-time, hobby project which involves hardware and I burnt a display in the process. That alone sends it to the bottom of the hobby projects triage I did wrong some stuff and learned how to it right (or at least differently) during that time  When I was younger I had a magic 8 ball and it was one of my favorite toys, the notion of having a pseudorandom assistance was quirky and the toy\u0026rsquo;s presentation itself has its mystique. Sadly, the liquid eventually evaporates and it breaks down with time.\nA couple of years ago I wanted to dwell into Arduino. As a programmer with no electronics knowledge (even now, as I\u0026rsquo;m just starting my career at college) it is a wonderful platform to develop solutions as there are hundreds of components available and a programmable ecosystem that enables people to make projects with little circuitry knowledge.\nI had an Arduino Uno, a display module and an accelerator module, so the idea pretty much came by itself, I wanted to emulate the 8 ball toy. The history of how I failed until I ended with the final prototype warrants a separate post so I will spare you the details, and will proceed to the implementation itself.\ntl;dr - Gimme the repo The sketchbook repo is available at https://github.com/lggomez/arduino-8ball\nSchematics This is the starting schematic of the project, which consists of the following components:\n Arduino Nano (optionally using an Arduino Uno board for development) sh1106 display module MPU6050 accelerometer module\nOr, more realistically, in the protoboard:\n  Working on the implementation High level algorithm The Arduino runs on a loop so any algorithm must be iterative (and absolutely makes sense, particularily for our case). The main flow is as follows:\n Initialize the display and accelerometer modules Calibrate the accelerometer Start main loop Read data from accelerometer If a shake was detected, increment counter If counter reaches threshold (let\u0026rsquo;s stay at 5) display a random 8ball message Reset counter and display Goto 4  Steps 1b and 4b (optional): Initialize and reset the watchdog timer. A watchdog is a parallel processor which will reset the Arduino, should the configured time threshold by the main loop pass. This is useful to have an automatic soft reset in case the Arduino hangs by a bug\nBefore going into the display or accelerometer details, let\u0026rsquo;s talk about memory.\nMemory usage - A small preface The Arduino Nano has the following memory specs:\n Flash (program) Memory: 32 KB of which 2 KB used by bootloader (leaving 30720 bytes) SRAM: 2 KB (2048 bytes)  So, we aren\u0026rsquo;t exactly working with a JVM heap in here. One must be careful with their dependencies (this includes the libraries used to handle to modules) and the code itself, notably string handling. As an example, most, if not all of the string literals used on the sketch are stored in program memory instead of the dynamic memory\nAnother potential pain point is something as simple as having debug information.\nThis is a comparison between the memory usage with full debug information:\nSketch uses 21108 bytes (68%) of program storage space. Maximum is 30720 bytes.\nGlobal variables use 1905 bytes (93%) of dynamic memory, leaving 143 bytes for local variables. Maximum is 2048 bytes.\nLow memory available, stability problems may occur.\n\u0026hellip; and with no debug information:\nSketch uses 19146 bytes (62%) of program storage space. Maximum is 30720 bytes.\nGlobal variables use 1799 bytes (87%) of dynamic memory, leaving 249 bytes for local variables. Maximum is 2048 bytes.\nLow memory available, stability problems may occur.\nDisplay - Porting fizzlefade I wanted to add display transitions between messages, and so I found this great article from Fabien Sanglard describing the original fizzlefade effect from Wolfestein 3D, and providing a C implementation. Porting this implementation to the Arduino nano, and specifically to the 128x64 sh1106 display provided some additional challenges:\n We\u0026rsquo;ll be using a single, full display page buffer. The tradeoff comes cited in the documentation of the library: Always redraw everything. It is not possible to redraw only parts of the content. The advantage is lesser RAM consumption compared to a full frame buffer in RAM. This last part of the sentence is rendered void for us, but for the following reason Making individual pixel writes to the device removes the memory limitations, but is is very slow and not feasible on the long run. Page size can also be smaller, but it won\u0026rsquo;t have a noticeable performance effect and it will, in fact, be slower sending 4 pages than performing a single flush of a larger page  With this in mind, I made an implementation with the following optimizations that proved to work:\n Initialize a full-size display buffer page Perform the pseudorandom pixel pickup with with the fizzlefade LFSR algorithm Every certain amount of iterations flush the buffer (write) to the screen. this includes the writing and centering of the string message as a \u0026lsquo;negative\u0026rsquo; on the previously written buffer  As a possible optimization, we could the string messages as a set of coordinates to avoid (or draw as a negative of the current display mode) instead of drawing the entire string on each buffer flush. But precalculating this is madness, and for storage reasons we would have to encode these in a form different than a sparse matrix (I would like to see the trade off against writing against the buffer, I doubt the effort required is worth it).\nThe display code can be seen below (sans debug output sections)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93  /* Modified fizzlefade implementation for Arduino integrated with u8g2 display See http://fabiensanglard.net/fizzlefade/index.php for details on the original Changes include: - Fade to a given text message - Send buffer to display every FIZZLEFADE_BUFFER_THRESHOLD iterations instead of doing it for each pixel to improve performance - Support for fill or clear mode */ void fizzlefade_message(char* charMessage, fizzlefade_mode mode) { u8g2.firstPage(); u8g2_uint_t fizzlefade_iterations; byte lineCount = getLineCount(charMessage); if (mode == clear) { // Fill the display at start if mode is clear  drawFill(); } do { uint32_t rndval = 1; u8g2_uint_t x, y; fizzlefade_iterations; // Start the fizzlefade LFSR loop  do { y = rndval \u0026amp; 0x000FF; /* Y = low 8 bits */ x = (rndval \u0026amp; 0x1FF00) \u0026gt;\u0026gt; 8; /* X = High 9 bits */ unsigned lsb = rndval \u0026amp; 1; /* Get the output bit. */ rndval \u0026gt;\u0026gt;= 1; /* Shift register */ if (lsb) { /* If the output is 0, the xor can be skipped. */ rndval ^= 0x00012000; } // If pixel is within bounds, count it as a batch iteration and start message print  if (x \u0026lt;= SH1106_WIDTH \u0026amp;\u0026amp; y \u0026lt;= SH1106_HEIGHT) { fizzlefade_iterations++; fizzle_message(x, y, charMessage, lineCount, mode, fizzlefade_iterations); } } while (rndval != 1); } while (u8g2.nextPage()); } // fizzle_message draws the random pixel provided and prints the message into the display buffer // if the iterations match FIZZLEFADE_BUFFER_THRESHOLD, the string will be writter and the // buffer will be written to the display void fizzle_message(u8g2_uint_t x, u8g2_uint_t y, char* charMessage, byte lineCount, fizzlefade_mode mode, u8g2_uint_t fizzlefade_iterations) { if (mode == clear) { u8g2.setDrawColor(0); } u8g2.drawPixel(x, y); if (fizzlefade_iterations % (FIZZLEFADE_BUFFER_THRESHOLD + 1) == FIZZLEFADE_BUFFER_THRESHOLD) { printMessage(charMessage, lineCount, mode); u8g2.sendBuffer(); } u8g2.setDrawColor(DEFAULT_DRAW_COLOR); } // printMessage prints a string message into the display page buffer, centering it // TODO: the centering is still a bit off to the left in certain cases void printMessage(char* charMessage, byte lineCount, fizzlefade_mode mode) { u8g2.setDrawColor((mode == clear) ? 1 : 0); char* buf; for (byte i = 1; i \u0026lt;= lineCount; i++) { buf = subStr(charMessage, \u0026#34;\\n\u0026#34;, i); byte x_offset = (strlen(buf) % 2) * floor(FONT_WIDTH / 2); byte x = (SH1106_WIDTH / 2) - (FONT_WIDTH * floor(strlen(buf) / 2)) - x_offset; byte y_offset = floor(1 - (0.5 * (lineCount - 1))) * FONT_HEIGTH; byte y = UPPER_MARGIN + (FONT_HEIGTH * i) + y_offset; u8g2.drawStr(x, y, buf); } } // getLineCount retuns the line count of a string given by the newline delimiter \\n byte getLineCount(const char* charMessage) { byte lineCount = 1; for (; *charMessage; charMessage++) lineCount += *charMessage == \u0026#39;\\n\u0026#39;; return lineCount; } // subStr returns a substring defined by a delimiter at an index char* subStr(char* str, char *delim, byte index) { char *act, *sub, *ptr; static char copy[MAX_MESSAGE_LENGTH]; byte i; // Since strtok consumes the first arg, make a copy  strcpy(copy, str); for (i = 1, act = copy; i \u0026lt;= index; i++, act = NULL) { sub = strtok_r(act, delim, \u0026amp;ptr); if (sub == NULL) break; } return sub; }   Accelerometer - Playing with IMUs As I would discover soon, this project tasked me with playing with IMU\u0026rsquo;s (Inertial Measurement Units) which by itself be easy, or difficult. Not because of the precision (in this case!) of the sensors, but mostly because of how the programmer wants to handle the sensor data. In the most elemental form, we are given the following data that compose the degrees of freedom (in this case 6) of the form:\n Acceleration: From the 3D space, in form of the vector ax, ay, az Gravity: Also from the 3D space, in form of the vector gx, gy, gz  The general algorithm for the 8 ball is as follows:\n Start sensor, calibrate it and start main loop, taking IMU measurements Detect the position and compare with the previous measurement If a shake was detected (that is, a perturbation was established well enough to classify the sensor state delta as one) increment the movement If some time was elapsed without any shakes, reset the counter (as it should be cumulative only over a small window of time) After a certain amount of shakes (say, 5), pick a random message and show it on display as explained on the previous section. Depending on how you want to consider the shakes (just a movement, or the combination or 2 movements: move up \u0026amp; move down) this part of the implementation may vary Reset shake counter and display Goto 2 (next main loop iteration)  The main algorithm itself isn\u0026rsquo;t very sophisticated, but the complicated question in this part is: what is movement? we have 2 vectors with instant measurements but a simple comparison won\u0026rsquo;t do it, and we must find a way to be able quantize and compare the state of the sensor.\nUsing filters In the end I used a complementary filter, which simplifies the measurement into pith and roll as units of measurement. Pieter Jan made an implementation and a post which I will just link here and recommend instead of replicating.\nAnalysing sensor output The Arduino IDE can easily help visualize the output of sensors via serial logging and its serial monitor tool. It only requires two things:\n Serial output must be of the form \u0026ldquo;label1:variable1, label2:variable2, \u0026hellip;\u0026rdquo; A separate printing macro should be used for this mode since any distinct output may corrupt the graph (and it doesn\u0026rsquo;t have a reset button yet)  1 - Standing still 2 - Slow, continued movements (no shaking) 3 - Some random movements 4 - Shake and message Notice the continued, consistent movements and the pause during the message\n5 - A bug? only first series of shakes detected Cannot reproduce it always but yeah, bugs happen (even on blog posts)\n\u0026hellip;and that\u0026rsquo;s it for now. There may still be things to iron out but I\u0026rsquo;m pretty happy with how this turned out, all things considered.\n","description":"Emulating an 8 ball toy in Arduino Nano","id":2,"section":"","tags":["Arduino Nano","C","Accelerometer","Display"],"title":"Emulating a magic 8 ball","uri":"https://luisgg.me/en/emulating_a_magic_8_ball/"},{"content":"For those unfamiliar with the concept of canary, let\u0026rsquo;s quote wikipedia first:\nCanaries were iconically used in coal mines to detect the presence of carbon monoxide. The bird\u0026rsquo;s rapid breathing rate, small size, and high metabolism, compared to the miners, led birds in dangerous mines to succumb before the miners, thereby giving them time to take action.\nAnd so, the term canary spread to other fields, including software development. Usually, before screwing things up in distributed systems, the new version is tested in a single node; this modus operandi is called a canary deploy (how to mediate the state changes between the new and old versions is completely out of the scope of this article, sadly). So, up to this point, the example is clear, a canary, conceptually, tests or measures something in case it goes awry.\nWhat happens with networks? networks are hard. Network configurations are hard. Network proxies are hard, VPNs are hard. Throw them all together in a mixed bag of custom configurations and you\u0026rsquo;re in for a bad time, especially when checking connectivity. Same happens with smart \u0026lt; insert your favorite appliance here \u0026gt;. Their networking stack and applications built on top of them, despite the resource constrains, are just not on the same league as the their PC (or even console, dare I say) counterparts. Why? It\u0026rsquo;s all in the release cycles.\nBut whatever, back to the point. I\u0026rsquo;ve found myself enough time in the awkward situation of \u0026ldquo;false positives\u0026rdquo; in regards of internet connectivity with my smart TV and my work PC, so I wondered, why not a network canary?\nTL;DR - I just want the repo https://github.com/lggomez/arduino-dns-canary\nThe design The idea behind this is simple: having a network device directly connected to the router, with a minimal network stack (in this case, the arduino board and its standard library via a W5500 ethernet controller)\nOn the software side, just a loop of TCP connections, cycling trough a list of well-known and always-up domains to test DNS resolution and measure connection time. That\u0026rsquo;s it.\nThe details themselves on the connections can be found online and in the repo but the diagram is self explainatory (albeit unreadable on the W5500). The busiest part is the W5500 connection which may depend on the specific vendor board you get, but the following reference should follow pretty closely:\nAs for the display (this thing needs an UI after all) I went for a 16x2 char display with an PCF8574T I2C front board, which made the connections a lot easier (and even then, I decided to leave the shield to mount the prototype on its case).\nThe prototype (Yeah, I noticed that input holes are not one sided. It\u0026rsquo;s a prototype, sue me).\nTip: for a poetic finish, nothing fits better than a crappy writting vaguely stating the purpose of the artifact.\nCode See the link above at TL;DR.\n","description":"Creating a network canary device with an Arduino Nano","id":3,"section":"","tags":["Arduino Nano","C","Ethernet","Display"],"title":"Making a network canary","uri":"https://luisgg.me/en/making_a_network_canary/"},{"content":"Hi there. Let\u0026rsquo;s talk about enumerations (commonly named enums) in golang.\nIn other languages, enums commonly work as a closed type (usually aliasing or composing the int and/or string primitives) to provide the user some additional flexibilites that constants do not provide:\n Aliasing: we can think of enums as constants, aliased on a type level Immutability: some languages work on numeric enums as numeric masks; howevers, their values are not altered during operations Extensibility: enums can (and have to be) easily extendable to the desired amount of values (at least with no extra burden on the developer on coding and compile time). After all, we can also think of them as a static collection of constants Value-safety: that is, an enum value is unique but can be unboxed into its primitive value Typed operations: as a bonus, language defined support provides free, out of the box method and operations like value checking and traversal\nGolang offers varied, but in my opinion, limited ways to implement enums.  Let\u0026rsquo;s see how messy we can get, depending on how many of the above guarantees we want.\nThe naive way: constants Start with the basics: after all, enum types are a group of constants of an integral/primitive type bound by another type (generically speaking). So, I wanna have my weekdays:\n1 2 3 4 5 6 7 8 9  const ( Sunday = \u0026#34;Sunday\u0026#34; Monday = \u0026#34;Monday\u0026#34; Tuesday = \u0026#34;Tuesday\u0026#34; Wednesday = \u0026#34;Tuesday\u0026#34; Thursday = \u0026#34;Thursday\u0026#34; Friday = \u0026#34;Friday\u0026#34; Saturday = \u0026#34;Saturday\u0026#34; )   The idiomatic way: constant aliases, a.k.a iota enums This is the classic and idiomatic way offered by the language to implement basic enumerations. In fact, the native time package offers a Weekday enum out of the box:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  type Weekday int const ( Sunday Weekday = iota Monday Tuesday Wednesday Thursday Friday Saturday ) // String returns the English name of the day (\u0026#34;Sunday\u0026#34;, \u0026#34;Monday\u0026#34;, ...). func (d Weekday) String() string { //... }   A couple (well, several\u0026hellip;) things to note here:\n The \u0026ldquo;enum\u0026rdquo; here already feels half baked, as it is a constant collection (given a syntactic shortcut) tied to a definition of a type alias Also, one must implement its Stringer interface if they want to give it a string representation (the language does not support infering it from the label) iota is a constant equal to zero. It is convention to use it as the first value, and the subsequent entries of the const block will have autoincremental values Under this convention, values lower than iota are assumed invalid. Consumers must adhere to this convention so this is already a leaky leanguage feature IMO, which depends on the good faith of the developers Given the above, do we validate values greater than iota upon type conversion? The only way to enumerate is to explicitly make a switch between the values  Hot topic Since I want to avoid copypasting code from users without context, here is a very interesting StackOverflow thread in which users go wild with their own ways to represent enumerations beyond the capabilities of the standard approach\nAnd it also it a popular issue in the official issue tracker, having several proposals standing:\n proposal: spec: add typed enum support - https://github.com/golang/go/issues/19814 proposal: Go 2: enums as an extension to types - https://github.com/golang/go/issues/28987 proposal: Go 2: exhaustive switching for enum type-safety - https://github.com/golang/go/issues/36387 \u0026hellip;and the list goes on: https://github.com/golang/go/issues?q=enum+label%3ALanguageChange+label%3AProposal+  And and even bigger list of enum generators, helpers, and implementations: https://github.com/search?l=\u0026amp;q=enum+language%3AGo\u0026amp;type=repositories\nThis includes mine somewhere, which I will start to discuss below.\nAdding the sugar ourselves with our custom type Having said that, what do I expect from an enum type to be compatible with in a large application or general use case?\n Immutability Ability to be traversable Enum fields compatible with several base and (un)marshal interfaces, including runtime validation:\na. Stringer\nb. json.Marshaler, json.Unmarshaler\nc. text.Marshaler, text.Unmarshaler\nd. json.Marshaler, json.Unmarshaler\ne. gob.GobEncoder, gob.GobDecoder\nf. driver.Valuer, sql.Scanner\ng. bson.Marshaler, bson.Unmarshaler (from go.mongodb.org/mongo-driver/bson package) Ability to perform type-safe comparisons at runtime against strings and instances of the same enum type  All of this sounds difficult to reutilize without any sort of code generation, so of course go generate was involved in the making. For this case, we will create a string enum type.\nCreating a core enum type The core enum type will have the job of keeping a registry of the exposed enumerations as a map of lookup maps. This is for two reasons:\n This enum type is designed to work on a package level, so ideally more than one enumeration will be expected here and have to be resolved Value lookups will be performed to validate incoming values un unmarshal operations  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  package enum import ( \u0026#34;strings\u0026#34; ) // Base value index for internal validations var ( enumIndex = map[string]valueIndex{} ) type valueIndex map[string]struct{} // stringEnumValue is a type designed to be embeddable in other structs so // they can expose type safe string enumerations and all of their // corresponding marshaling and unmarshaling operations // // This type is NOT meant to be consumed directly type stringEnumValue struct { value string key string } func fromValue(value string, ignoreCase bool, key string) (stringEnumValue, bool) { if index, ok := enumIndex[key]; ok { if ignoreCase { value = strings.ToUpper(value) } for v := range index { if ignoreCase { v = strings.ToUpper(v) } if v == value { return stringEnumValue{v, key}, true } } } return stringEnumValue{}, false } // Equals does a case sensitive comparison against a string value func (e stringEnumValue) Equals(s string) bool { return e.value == s } // Equals does a case insensitive comparison against a string value func (e stringEnumValue) EqualsIgnoreCase(s string) bool { return strings.ToUpper(e.value) == strings.ToUpper(s) } // IsEmpty returns whether the value is empty. // Such values are permitted in order to support empty values on compile and (un)marshaling time without pointer fields func (e stringEnumValue) IsEmpty() bool { return e.value == \u0026#34;\u0026#34; } // IsUndefined returns whether the value is manually initialized, thus being undefined (MyEnumType{}). // Such values are permitted in order to support empty values on compile and (un)marshaling time without pointer fields func (e stringEnumValue) IsUndefined() bool { return e.IsEmpty() \u0026amp;\u0026amp; e.key == \u0026#34;\u0026#34; }   Codecs: handling validation upon marshalling Here we implement the support for all the marshallers/serializers we want on the enum support, validating that the incoming value is indeed one of the enumeration constants, otherwise returning a marshalling validation error.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133  package enum import ( \u0026#34;database/sql/driver\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/bson\u0026#34; ) // codecs: marshal/unmarshal methods for several native interfaces: // - Stringer // - json.Marshaler, json.Unmarshaler // - text.Marshaler, text.Unmarshaler // - bson.Marshaler, bson.Unmarshaler // - json.Marshaler, json.Unmarshaler // - gob.GobEncoder, gob.GobDecoder // - driver.Valuer, sql.Scanner const ( JSONNull string = \u0026#34;null\u0026#34; ) // Validate and assign value to unmarshal into target enum instance. // // Since we know the specific type by the key injected in e.key, we // check against the enum index to validate the incoming value func (e *stringEnumValue) validateValueByKey(value string) error { if _, ok := enumIndex[e.key][value]; !ok \u0026amp;\u0026amp; (value != \u0026#34;\u0026#34;) { return fmt.Errorf(\u0026#34;stringEnumValue: value \u0026#39;%v\u0026#39; is not allowed\u0026#34;, value) } return nil } // Stringer implementation func (e stringEnumValue) String() string { return e.value } // MarshalJSON returns the stringEnumValue value as JSON func (e stringEnumValue) MarshalJSON() ([]byte, error) { data, err := json.Marshal(e.value) return data, err } // UnmarshalJSON sets the stringEnumValue value from JSON func (e *stringEnumValue) UnmarshalJSON(data []byte) error { var value string if err := json.Unmarshal(data, \u0026amp;value); err != nil { return err } if err := e.validateValueByKey(value); err != nil { return err } e.value = value return nil } // UnmarshalText parses a text representation into a date types func (e *stringEnumValue) UnmarshalText(text []byte) error { value := string(text) if err := e.validateValueByKey(value); err != nil { return err } e.value = value return nil } // MarshalText serializes this date types to string func (e stringEnumValue) MarshalText() ([]byte, error) { data := []byte(e.String()) return data, nil } // Scan scans a stringEnumValue value from database driver types. func (e *stringEnumValue) Scan(raw interface{}) error { switch v := raw.(type) { case []byte: return e.UnmarshalText(v) case string: return e.UnmarshalText([]byte(v)) default: return fmt.Errorf(\u0026#34;cannot sql.Scan() enum.stringEnumValue from: %#v\u0026#34;, v) } } // Value converts stringEnumValue to a primitive value ready to written to a database. func (e stringEnumValue) Value() (driver.Value, error) { return driver.Value(e.String()), nil } // MarshalBSON implements the bson.Marshaler interface. func (e stringEnumValue) MarshalBSON() ([]byte, error) { return bson.Marshal(bson.M{\u0026#34;data\u0026#34;: e.String()}) } // UnmarshalBSON implements the bson.Unmarshaler interface. func (e *stringEnumValue) UnmarshalBSON(data []byte) error { var m bson.M if err := bson.Unmarshal(data, \u0026amp;m); err != nil { return err } if data, ok := m[\u0026#34;data\u0026#34;].(string); ok { if err := e.validateValueByKey(data); err != nil { return err } e.value = data return nil } return errors.New(\u0026#34;couldn\u0026#39;t unmarshal bson bytes string as enum.stringEnumValue\u0026#34;) } // GobEncode implements the gob.GobEncoder interface. func (e stringEnumValue) GobEncode() ([]byte, error) { return e.MarshalBinary() } // GobDecode implements the gob.GobDecoder interface. func (e *stringEnumValue) GobDecode(data []byte) error { return e.UnmarshalBinary(data) } // MarshalBinary implements the encoding.BinaryMarshaler interface. func (e stringEnumValue) MarshalBinary() ([]byte, error) { return []byte(e.value), nil } // UnmarshalBinary implements the encoding.BinaryUnmarshaler interface. func (e *stringEnumValue) UnmarshalBinary(data []byte) error { value := string(data) if err := e.validateValueByKey(value); err != nil { return err } e.value = value return nil }   Implementing the enumeration On the other side, we have the concrete enum type we want. This type is mostly a public facade with 3 tasks:\n Registering itself into the enumeration index of the local package on init() Proxying calls to the codecs method implementations Define its iterator and public types, values\nNote that to achieve immutability the public members have to be func() Weekday types rather than just Weekday, as anyone outside the package could just take its reference and mutate it otherwise  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121  package enum import ( \u0026#34;fmt\u0026#34; ) /** * * init: register enum to the internal index * **/ //nolint:gochecknoinits // enum has to register itself for unmarshaling at runtime func init() { if _, ok := enumIndex[weekdayKey]; ok { panic(fmt.Sprintf(\u0026#34;enum: enumeration %s is already registered\u0026#34;, weekdayKey)) } enumIndex[weekdayKey] = weekdayValueIndex } /** * * Type aliases and declarations * **/ type Weekday struct { stringEnumValue } func WeekdayFromValue(value string, ignoreCase bool) (Weekday, bool) { result, found := fromValue(value, ignoreCase, weekdayKey) return Weekday{result}, found } type weekdayList []func() Weekday // weekdayEnum is a type and memory safe iterable enumeration of Weekday values type weekdayEnum struct { weekdayList } func (e weekdayEnum) ForEach(f func(int, Weekday)) { for i, e := range e.weekdayList { f(i, e()) } } func (e weekdayEnum) Len() int { return len(e.weekdayList) } /** * * Private value index, key * **/ var ( weekdayValueIndex = valueIndex{ \u0026#34;Friday\u0026#34;: {}, \u0026#34;Monday\u0026#34;: {}, \u0026#34;Saturday\u0026#34;: {}, \u0026#34;Sunday\u0026#34;: {}, \u0026#34;Thursday\u0026#34;: {}, \u0026#34;Tuesday\u0026#34;: {}, \u0026#34;Wednesday\u0026#34;: {}, } weekdayKey = \u0026#34;Weekday\u0026#34; ) /** * * Public enumeration * **/ var ( WeekdayFRIDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Friday\u0026#34;, weekdayKey}} } WeekdayMONDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Monday\u0026#34;, weekdayKey}} } WeekdaySATURDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Saturday\u0026#34;, weekdayKey}} } WeekdaySUNDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Sunday\u0026#34;, weekdayKey}} } WeekdayTHURSDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Thursday\u0026#34;, weekdayKey}} } WeekdayTUESDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Tuesday\u0026#34;, weekdayKey}} } WeekdayWEDNESDAY = func() Weekday { return Weekday{stringEnumValue{\u0026#34;Wednesday\u0026#34;, weekdayKey}} } EnumWeekday = weekdayEnum{weekdayList{ WeekdayFRIDAY, WeekdayMONDAY, WeekdaySATURDAY, WeekdaySUNDAY, WeekdayTHURSDAY, WeekdayTUESDAY, WeekdayWEDNESDAY, }} ) /** * * Proxy methods for enum unmarshaling * **/ func (e *Weekday) UnmarshalJSON(data []byte) error { e.key = weekdayKey return e.stringEnumValue.UnmarshalJSON(data) } func (e *Weekday) UnmarshalText(text []byte) error { e.key = weekdayKey return e.stringEnumValue.UnmarshalText(text) } func (e *Weekday) UnmarshalBSON(data []byte) error { e.key = weekdayKey return e.stringEnumValue.UnmarshalBSON(data) } func (e *Weekday) UnmarshalBinary(data []byte) error { e.key = weekdayKey return e.stringEnumValue.UnmarshalBinary(data) } func (e *Weekday) GobDecode(data []byte) error { e.key = weekdayKey return e.stringEnumValue.GobDecode(data) } func (e *Weekday) Scan(raw interface{}) error { e.key = weekdayKey return e.stringEnumValue.Scan(raw) }   Using the generator I won\u0026rsquo;t bore you with the details of code generation, and for that end I already made one for this enum implementation avilable at github.com/lggomez/go-enum documentation and examples included.\n","description":"The state of enums in go and how to create a self validating type generator","id":4,"section":"","tags":["Golang","Enums","Scaffolding"],"title":"The otherworldly landscape of enums in go","uri":"https://luisgg.me/en/the_otherworldly_landscape_of_enums_in_go/"},{"content":"As mentioned after the fact, this site is built from a zim notebook and RSS generation is not one of the features included in zim as of today, so I immediately went into researching how to achieve this in the most straightforward but automatic way possible.\nSome notes on zim format Zim notebooks are file tree structures and tipically consist of 3 things:\n The zim notebook definition file, notebook.zim, which is located at the root directory. The format of this file is the same as .ini files, containing a main [Notebook] key with the notebook metadata and additional top level entries for plugins The wiki pages themselves, on txt format (at least by default). These files have an unique format with metadata present in the first 6 lines of the file The directories which contain subentries. These directories are accompanied by an equally named page, so that the directory is present in the wiki structure itself  Some notes on RSS The complete RSS structure and documentation can be read here, but sometimes a code snippet and some bullets say more than a thousand words:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; ?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Main site title\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://www.foo.com\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;Site description\u0026lt;/description\u0026gt; \u0026lt;managingEditor\u0026gt;mail@example.com (Author Name)\u0026lt;/managingEditor\u0026gt; \u0026lt;pubDate\u0026gt;Wed, 23 Dec 2020 21:19:19 -0300\u0026lt;/pubDate\u0026gt; \u0026lt;image\u0026gt; \u0026lt;url\u0026gt;https://www.foo.com/icon.gif\u0026lt;/url\u0026gt; \u0026lt;link\u0026gt;https://www.foo.com/index.php\u0026lt;/link\u0026gt; \u0026lt;/image\u0026gt; \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;Article 1\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://www.foo.com/article1.html\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;item description\u0026lt;/description\u0026gt; \u0026lt;author\u0026gt;Author Name\u0026lt;/author\u0026gt; \u0026lt;pubDate\u0026gt;Fri, 15 Jan 2021 20:34:28 -0300\u0026lt;/pubDate\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;Article 2\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://www.foo.com/article2.html\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;item description\u0026lt;/description\u0026gt; \u0026lt;author\u0026gt;Author Name\u0026lt;/author\u0026gt; \u0026lt;pubDate\u0026gt;Fri, 17 Jan 2021 21:35:29 -0300\u0026lt;/pubDate\u0026gt; \u0026lt;/item\u0026gt; \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt;    Format is XML (to dispel any remaining doubts) Top level element channel contains the entire feed Sub-elements are classified in two types:  Site metadata attributes Item attributes, which are the articles themselves and their metadata   Additionally, items can contain the entire article via the  tag, but since we assume the content we\u0026rsquo;re dealing with still hasn\u0026rsquo;t been scaffolded to HTML the usage of this field is outside the scope of this project  Putting stuff together in go The aim is to do the scaffolding in 3 steps:\n Notebook traversal \u0026amp; metadata retrieval RSS entity generation XML file generation  For reading the notebook file and generating the RSS feed respectively, the following packages were used (and very useful):\n github.com/gookit/ini/v2 github.com/gorilla/feeds  Also, github.com/jessevdk/go-flags always comes in handy for basic commandline parsing on tagged structs\nThe traversal becomes very simple by using the filepath.Walk function, which does a lexical BFS traversal of the directory (which makes it deterministic but may suffer performance-wise on dense directory structures, which is not our case). The main code is the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  func traverseAndParsePageMetadata(rootPath string, notebookPath string) zim.PageMetadataByCreationDate { pages := zim.PageMetadataByCreationDate{} walkErr := filepath.Walk(rootPath, func(path string, info os.FileInfo, err error) error { if path == notebookPath { return nil // ignore base notebook file \t} if err != nil { return err } if !info.Mode().IsRegular() { return nil } if !info.IsDir() { // TODO: Detect and ignore empty pages? (with empty content starting from line 7) \tpageMetadata, err := zim.ParsePage(path, info) if err != nil { return err } pages = append(pages, pageMetadata) } return nil }) if walkErr != nil { panic(walkErr) } return pages }   Then, RSS generation is similarilly simple:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  func createRSSFeed(pages zim.PageMetadataByCreationDate, rootPath string, zimNotebook zim.Notebook) (string, error) { // Create main feed element \tfeed := \u0026amp;feeds.Feed{ Title: commandlineFlags.Title, Link: \u0026amp;feeds.Link{Href: commandlineFlags.Link}, Description: commandlineFlags.Description, Author: \u0026amp;feeds.Author{Name: commandlineFlags.AuthorName, Email: commandlineFlags.AuthorEmail}, Created: pages[0].CreationDate.Add(-(60 * time.Minute)), // Use 1 hour prior to the first page creation as an arbitrary creation date \t} items := make([]*feeds.Item, 0, len(pages)) for _, p := range pages { pageLink := p.PathToURL(rootPath, commandlineFlags.Link, zimNotebook.DefaultFileExtension) items = append(items, \u0026amp;feeds.Item{ Title: p.Title, Link: \u0026amp;feeds.Link{Href: pageLink}, //Description: , TODO: no description available from page so this will be filled manually on generated file \tAuthor: \u0026amp;feeds.Author{Name: commandlineFlags.AuthorName, Email: commandlineFlags.AuthorEmail}, Created: p.CreationDate, }) } feed.Items = items rss, err := feed.ToRss() if err != nil { log.Fatal(err) } return rss, nil }   Finally, creating the file is just a matter of saving the return value of createRSSFeed\u0026hellip;\n1 2 3 4 5 6 7  // Write RSS feed file \tf, err := os.Create(\u0026#34;rss.xml\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() f.WriteString(rss)   \u0026hellip; and that\u0026rsquo;s it, now we have and RSS feed file. All that remains is adding any additional info that we want (like the article descriptions or the site image).\nThe commandline application\u0026rsquo;s code is available at https://github.com/lggomez/go-zimrss for anyone interested.\n","description":"How to generate RSS feeds for zim notebooks","id":5,"section":"","tags":["Golang","Post-mortem"],"title":"Scaffolding RSS feeds from zim notebooks","uri":"https://luisgg.me/en/scaffolding_rss_feeds_from_zim_notebooks/"},{"content":"Drama isn\u0026rsquo;t usual in the go community as far as I\u0026rsquo;m concerned and I wouldn\u0026rsquo;t call this incident itself a drama but it reminds me of instances that happen more than often on other technology stacks [citation omitted].\nSecurity utilities are no joke, and no one in their sane mind would go about writing a custom RSA implementation in C++ for, say, their bank employer. People dedicate their entire careers writing this stuff and any modern platform or programming language has a security toolset for developing applications, and, otherwise, the open source solutions which are battle-tested by the community offer more guarantee that a one-man solution. Which brings us to today\u0026rsquo;s entry.\nGo contains several official packages related to security but, to my surprise when I looked around, there was no canonical JWT implementation. I needed to do some token verification and introspection, and I decided to go with https://github.com/dgrijalva/jwt-go as it seems to be one of the most popular packages with an API that looked pretty ergonomic for my use case.\nHowever, as of today it still has a security flaw: https://github.com/dgrijalva/jwt-go/issues/428\nThe repo is not maintained anymore (altough is stable aside from that issue) and the owner is nowhere to be found, or neither wants to be found. In the context of dependency management, this problem brings up some questions:\n Is it viable to use a fork at the cost of fragmentation? What happens to versions which were already audited by some process? a fork and new owner implies a full re-audit of the dependency? Why is google in the list of users of this package and didn\u0026rsquo;t bother to do a fork, or better yet, an official package like in the case of oauth?  Really interesting questions but for which I didn\u0026rsquo;t have the answers, or the time to find them as I wanted to finish my implementation. Moving to another, more interesting question, how much of the package would I need to copy or re-implement in order to fix this without doing a complete fork or having to move on to another package?\nNot so tight, not so loose coupling The basic workflow goes like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import ( jwtgo \u0026#34;https://github.com/dgrijalva/jwt-go\u0026#34; ) claims := jwtgo.MapClaims{} // Traverse JWKS and validate/match by kid (ideally) for _, key := range publicKeys { token, err := jwtgo.ParseWithClaims(rawStringJWT, \u0026amp;claims, func(t *jwtgo.Token) (interface{}, error) { // Revalidate signing method to prevent bypass attacks  // ...  return key.PublicKey, nil // PublicKey is my *rsa.PublicKey instance containing the valid key to verify the token  }) }   The issue is in the jwtgo.MapClaims type, which contains the bug, but wait: the ParseWithClaims method expects a Claims parameter, which is an interface:\n1 2 3  type Claims interface { Valid() error }   Extending The So, knowing this, we can implement our custom type, namely a copy of the fixed implementation of the jwtgo.MapClaims type (for reference, see https://github.com/dgrijalva/jwt-go/pull/385). This type complies with Claims interface and can be extended to advanced validations, like required and optional claims:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  type MyFixedClaims = jwtgoclaims.MapClaims // alias to subpackage containing the fixed claims implementation // for clarity and easier replacement later on if the author // is alive and decides to merge the pull request  type ClaimValue struct { Value string Required bool } type ClaimsVerificationValues map[string]ClaimValue func VerifyMyClaims(myClaims MyFixedClaims, verifier ClaimsVerificationValues) error { for k, v := range verifier { if err := verifyValue(myClaims, v.Value, k, v.Required); err != nil { return err } } } func verifyValue(c MyFixedClaims, expected, key string, required bool) error { val, found := c[key] actual, ok := val.(string) if !(found \u0026amp;\u0026amp; ok) \u0026amp;\u0026amp; required { return fmt.Errorf(\u0026#34;%v verification failed\u0026#34;, key) } if required { if !ok || actual == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;%v verification failed\u0026#34;, key) } } else if actual == \u0026#34;\u0026#34; { return nil } if actual != expected { return fmt.Errorf(\u0026#34;%v verification failed\u0026#34;, key) } return nil }   I\u0026rsquo;d like to think there is a nicer way of doing the last method, so I will leave that up to the reader.\nWrapping up With that, our initial code piece would end up like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import ( jwtgo \u0026#34;https://github.com/dgrijalva/jwt-go\u0026#34; ) claims := MyFixedClaims{} verificationValues := ClaimsVerificationValues{ \u0026#34;customclaim\u0026#34;: ClaimValue{Value:\u0026#34;foo\u0026#34;, Required: true}, \u0026#34;konamicode\u0026#34;: ClaimValue{Value:\u0026#34;↑ ↑ ↓ ↓ ← → ← → B A\u0026#34;, Required: false} } // Traverse JWKS and validate/match by kid (ideally) for _, key := range publicKeys { token, err := jwtgo.ParseWithClaims(rawStringJWT, \u0026amp;claims, func(t *jwtgo.Token) (interface{}, error) { // Revalidate signing method to prevent bypass attacks  // ...  if err := VerifyMyClaims(claims, verificationValues); err != nil { return nil, err } // ...  return key.PublicKey, nil // PublicKey is my *rsa.PublicKey instance containing the valid key to verify the token  }) }   Drama is avoided. Pending refactor is all that remains.\nPS: Be nice and don´t forget to include the licenses belonging to the code you fork when applicable.\n","description":"How to keep using jwt-go without forking or deprecating it in your project","id":6,"section":"","tags":["Golang","JWT"],"title":"Fixing and extending jwt-go without forking","uri":"https://luisgg.me/en/fixing_jwt_go_without_forking/"},{"content":"Once every couple of months or years, a developer tend to stumble upon a bug that leaves them completely stumped. The cause is not apparent, it cannot be reproduced consistently (or the conditions are rather bizarre and seemingly unrelated to the bug itself) and weeks or months can be spent tinkering on it (because your boss obviously only cares about the business: if fixing it doesn\u0026rsquo;t generate money it isn\u0026rsquo;t broken).\nIn 2019 I had the opportunity to enjoy this experience with what appeared to be a bug in one of the backend services I was developing at that time. Eventually, it unfolded into a month-long journey for me just to pinpoint the issue on my end and a three-month adventure for the compiler engineer/scientist from google who took upon the task of fixing the issue. Happily, the fix landed on go 1.14 (no backports sadly, as it would break literally a lot of stuff otherwise).\nWhere\u0026rsquo;s the issue? In my backend service I introduced a nil dereference but that was far from being the worse thing. There was something odd about it: the stacktrace showed a place where it could not happen. Not locally, at least. Neither remotely, after adding logs, but at which point the offending line was correctly displayed, so I immediately suspected of my code (why would I do otherwise?).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  func (ls LocationsMutableService) getChildren(ctx context.Context, tree string, loc string) (LocationChildren, error) { url := \u0026#34;\u0026#34; var children LocationChildren //line 59 - panic not possible here \tresponse, err := ls.Client.GET(url, \u0026amp;children) //line 61 - actual place of panic (response == nil) \tif response.StatusCode == http.StatusNotFound { return nil, errors.New(\u0026#34;\u0026#34;) } if err != nil { return nil, err } return children, err }   Rabbit\u0026rsquo;s Hole B1 - Our panic handler We had a rather simplistic fork of the gingonic recovery handler (which does some questionable things like inadvertently inducing file I/O per request, although locally) but the main stack frame handling code remained intact from the gin codebase, so I looked up there. At the same time, we also used the newrelic agent for stacktrace and error noticing, so the behavior was the same (both were informing the wrong line, under this specific code).\nI proceeded to rewrite the recovery handler using the debug.Stack() method instead of doing the manual traversal of PCs (program counters) via runtime.Callers(), to see if anything changed. To my surprise, it solved the issue. I wasn\u0026rsquo;t the best way (since debug.Stack() spits a byte slice with the already formed stacktrace) but as long as the correctness was assured, I was willing to do the sacrifice. The bug was surely in gingonic\u0026rsquo;s code\u0026hellip; right.\nOh, there was also another point of failure now\nRabbit\u0026rsquo;s Hole B2 - NewRelic\u0026rsquo;s agent Enter https://github.com/newrelic/go-agent/issues/100\nThey were using the same mechanism, runtime.Callers(), to construct the stacktrace, so I had to open an issue there and discuss it. Some comment arouse about the runtime, but it still couldn\u0026rsquo;t be an issue on their end\u0026hellip; right?\nRabbit\u0026rsquo;s Hole B3 - The go runtime Enter https://github.com/golang/go/issues/34123\nDiagnosing a runtime issue of this nature can be an herculean work. For starters, the following factors limited reproducibility:\n The inner workings of the go compiler and runtime at work with this bug were (and still are) beyond my capabilities, so I was completely unable to isolate the issue to a piece a code I could share with the golang team Any change to the affected package had a great change of making the bug disappear, so I was stuck with a git commit Due to how the web APIs were designed at the company, these couldn\u0026rsquo;t be executed out of the box, so I was stuck with a docker image of said git commit (docker became my friend here)  So, with the bug and starting assumption that debug.Stack() and runtime.Callers() PCs differ on panic trigger sites, the following steps were taken:\n Get to the bottom and the source of the debug.Stack() and runtime.Callers() functions. Interestingly, they share some common code but in completely different ways of use (please note that these revisions are up to date, so implementation details from the original issue may be lost). On the botton there is the traceback runtime helper, which provides the internal stack frame buildup and traversal utilities Provide the output of -S on the LocationsMutableService.getChildren function. Build with go build -gcflags=-S, save the output, find the code for that function, and paste it here. Disassemble the binary (objdump -d), find the same function above, and paste the disassembly of it. Show the actual values in the runtime.Callers return slice. Make sure to use the exact binary used in the previous 2 points (i.e. if you hack in some print statements for this, use the binary with the print statements in it when disassembling).  Off-by-one After some experimentation and log hacking into the runtime code, I found out some additional clues:\n This issue happens from go 1.12 and afterwards, including 1.13. 1.11 seems unaffected (can\u0026rsquo;t say anything about prior versions) It can be observed that PCs are being decremented on traceback.callers when it shouldn\u0026rsquo;t, but only in some specific cases like the repro I have at work (and, for the time being, only on the panic site as we don\u0026rsquo;t any any proof that suggests otherwise) After some history bisection I discovered that some heavy work was done on tracebacks for 1.12 which involves how PCs are manipulated, this particular PR stood out since the description and changes seem directly related to this issue: 232c979#diff-5a2de8a1053d4e11fbc71407a5361e93  With all of this info together, one of the compiler engineers of the golang team started to identify and work on the issue (it was an off-by-one bug with program counter handling, but not a trivial one). And another one came with a simple (?) and isolated repro:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  // run // Copyright 2019 The Go Authors. All rights reserved. // Use of this source code is governed by a BSD-style // license that can be found in the LICENSE file. // Make sure that the line number is reported correctly // for faulting instructions. package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; ) var x byte var p *byte //go:noinline func f() { q := p x = 11 // line 23 \t*q = 12 // line 24 } func main() { println(runtime.Version()) defer func() { recover() var pcs [10]uintptr n := runtime.Callers(1, pcs[:]) frames := runtime.CallersFrames(pcs[:n]) for { f, more := frames.Next() if f.Function == \u0026#34;main.f\u0026#34; \u0026amp;\u0026amp; f.Line != 24 { panic(fmt.Errorf(\u0026#34;expected line 24, got line %d\u0026#34;, f.Line)) } if !more { break } } }() f() }   And the long awaited fix: https://go-review.googlesource.com/c/go/+/196962/2#message-1b08a694cb1ee65964a69e436361f75eb5c8fe67.\nIt was a wild ride. I liked dwelling into this (and I just love the fact that you can just hack away into your local go code base) but I can live another year or two before having to hope on another ride like this one.\nConclusion I will gladly quote the cigarette man from the X-Files on this: \u0026ldquo;trust no one\u0026rdquo;.\nYour code will always be the first suspect but always remember that every link of the chain has and will have bugs, and if things go awry and you code seems completely out of the loop on the issue maybe it\u0026rsquo;s not the first place to look at.\n","description":"A matryoshka of bugs ends up in golang itself","id":7,"section":"","tags":["Golang","Post-mortem","Runtime","Compilers"],"title":"That one with the go runtime bug","uri":"https://luisgg.me/en/that_one_with_the_go_runtime_bug/"},{"content":"This is a public post-mortem that was intended to be released on the engineering blog of MercadoLibre, but my departure (and the slowness of the editing) prevented that.\n(This post is the full version of my Go meetup Buenos Aires 2019 II presentation).\nGoing from apicall to in-memory transaction As a part of the job I was doing at the time to scale the shipping delivery calculator, one of the next steps involved migrating an API (route coverage) to an in-memory mixed model. In other words, we had to store in memory all the documents we were reading from our ElasticSearch cluster.\nWhat I didn\u0026rsquo;t know is that a benchmarking adventure would await, and that the culprit was something I could\u0026rsquo;ve never taken into account until that moment.\nWhat happened? Usually the first steps on for a task of this type consists on developing a proof of concept project with mocked data in the in-memory storage. Once the build passed and some sanity tests were performed, we proceeded to perform the stress test, and that\u0026rsquo;s when things got weird:\nThe spikes immediately point out to the GC (garbage collector), and almost confirm it as the culprit if you observe them: the first curve belongs to the in-memory repo initialization, and the next spikes belong to the GC pauses induced by incoming traffic.\nThere were no suspicious things on the memory side so we had to tread more carefully with the help of pprof, and a gingonic handler which exposes its endpoints with some extra facilities (https://github.com/DeanThompson/ginpprof).\nThis one is a particular case: we don\u0026rsquo;t have directly observable heap problems but we do have heavy pauses and/or latency on the request times, the the last time goes on the trace.\nTrace: that cool but introverted friend Some parts of the go tooling are both a blessing and a curse. While being very powerful to the developer, the commandline API and its documentation (is there is any at all) leaves a bit to be desired, sometimes being a bit esoteric and hard to follow.\nI\u0026rsquo;d dare to say that this is the case with some of the most specific tools of the profiling toolchain in go like pprof and trace, but fortunately the community comes to the rescue!.\nFor the case of go case and the execution tracer (graphic frontend for go trace files) there is a fantastic tutorial made by Rhys Hiltner: https://about.sourcegraph.com/go/an-introduction-to-go-tool-trace-rhys-hiltner.\nTrace crach course Get an application\u0026rsquo;s trace:  Turn on and expose pprof on said up (as mentioned above with https://github.com/DeanThompson/ginpprof) run curl http://my-app.com/debug/pprof/trace?seconds=60 \u0026gt; foo ??? Profit! once those 60 seconds passed, we\u0026rsquo;ll have the trace file named foo downloaded containing the profile data for our running application  Visualize a trace file:  run go tool trace foo ??? Profit? maybe\u0026hellip; time to start looking for stuff now\nWith these tools we can start investigating  Preliminary results First culprit - Garbage Collector Upon seeing the trace the first observation is that a STW (stop the world) GC cycle takes a long time (164 milliseconds), and we also see an abnormal increase in the goroutine count (probable expected due to the still incoming flux of requests to the application). This API version does no explicitly create goroutines at no point in any hot path so we suspect there is a contention issue, which would explain the timing increase and also the increase of the blocking GC mark cycles:\nAnother crucial information is that the CG trigger on all instances is logrus (the logging package used by our toolkit):\nSecond culprit - Logger/logrus To discover the root cause of these contention spikes, I proceeded to check and compare the trace\u0026rsquo;s Synchronization blocking profile and Scheduler latency profile sections, which provided valuable information.\nThe contention in this scenario came from a mix of goroutines taking ownership of a mutex vs the unlock being parked by the runtime scheduler. We can see it by comparing the blocking and synchronization profiles:\nThird culprit - runtime and instance Another, more down to earth reason of the application\u0026rsquo;s degradation is that the go runtime is using just 2 OS threads to serve goroutines. This is defined in go 1.x by the environment variable GOMAXPROCS, and in the trace itself is given by the number of procs recorded:\nAnd for further confirmation, you can log the value returned by runtime.GOMAXPROCS(0), which in this case will be equal to 2. The setup and benchmarking of this variable is beyond the scope of this article, so as last detail I will add that by default the go runtime uses the number of logical CPU cores as its value.\nNow that we have the data, it is time to start fiddling with the code\nFirst round - The logger Logrus, effectively serializes its writes via a mutex, workflow which comes turned on by default. On a readme section, it can be noted that it can be disabled on some scenarios, so that was my next step, and, surprisingly, things got even worse than before:\nThis time, the contention and GC times went way up 50~100x and 7x respectively).\nHey, what happened? Going back to the trace profiles, I see that the writes keep serializing, but this time using a different kind of semaphore:\nCure via amputation Upon seeing this, I realized the easiest way to confirm the logger as the (triggering) root cause was to remove it from all hot paths and voilà:\nNow these are the numbers we all wanna see on an in-memory API, don\u0026rsquo;t we?\nBut still, I kinda want a more satisfactory explanation for this issue and I\u0026rsquo;m already halfway the rabbit hole, so here I go\u0026hellip;\nSecond round - Locks As it turns out, locks are one of those things whose implementation may seem easy(er) on a CS course level but once we talk about runtimes and operative systems we get into the \u0026lsquo;here be dragons\u0026rsquo; territory very quickly, always in a huge mix of trade-offs and complex use cases.\nAs we saw before, we dealt with two different mutex variants, so it\u0026rsquo;s worth taking a look at the performance implications of each one\nThe traditional sync.Mutex does a whole lot of stuff, including the following:\n Using internal sync primitives to handle internal runtime semaphores Handling the necessary spin and CAS operations to contain goroutines competing to acquire the lock It has a mixed operation mode: by default the ownership passage is FIFO + incoming goroutines, and if the acquisition time exceeds 1 millisecond for any waiter the mutex goes into starvation mode. Under this mode, several heuristics ensure that all waiters get served and mitigates tail latency, but global wait times increase and performance can degrade if the mutex operates consistently under this mode  The last point was key contributor on the benchmarked issues\nOn the other side, I found the internal fdMutex, which is a specialized mutex that serializes file descriptor accesses. If we cite the code:\n1 2 3 4 5 6 7 8 9 10  poll: // Package poll supports non-blocking I/O on file descriptors with polling. // This supports I/O operations that block only a goroutine, not a thread. // This is used by the net and os packages. // It uses a poller built into the runtime, with support from the // runtime scheduler. poll/fdMutex // fdMutex is a specialized synchronization primitive that manages // lifetime of an fd and serializes access to Read, Write and Close // methods on FD.   It\u0026rsquo;s beyond saying that the I/O cost in the context of a goroutine exceeds the penalty of the previous in-memory lock. The case of avoiding serializations in memory inadvertently causing to increment the amount of I/O syscalls can be worse than the original issue.\nFinal round - The Garbage Collector If you got to this point you had seen a lot of mentions to the GC in the traces and the article, and that\u0026rsquo;s the hill this article dies on\nGolang\u0026rsquo;s garbage collector is a very complex beast that takes a lot of work from us at cost of trade-offs that should be transparent to application developers, altough this doesn' always end that way.\nI will paraphrase the CoreCLR preamble (which is also inspired on The Garbage Collection Handbook):\n GC cycles should occur with enough frequency so that the heap isn\u0026rsquo;t overrun with unused objects (thrash) but at the same time occupying the least amount of CPU time GC cycles should be productive. If the GC reclaims a small amount of memory, both the GC cycle and the CPU time are wasted Each GC cycle must be fast. A lot of workloads have low latency requirements Managed code developers should not know much about the GC in order to reach a good memory usage from their code. The GC should adjust to satisfy different memory usage patterns  Author\u0026rsquo;s corollary: any sufficently bad algoritm will invalidate any of the previous points\nThe Go GC is (loosely) based on the following principles:\n Mark and Sweep: Objects which are reachable by the program are marked as in-use. This demands traversing the object heap periodically Tri-color algorithm: 3 rotary colors are used to determine the reachability state and marking the of objects: white, grey and black Concurrent execution: With the exception of STW pauses on certain phases, the GC runs concurrently with the rest of the application  The GC life cycle is formed by the following steps:\n Sweep termination (STW) Mark phase Mark termination (STW) Sweep phase once enough memory is alocated (GC trigger) goto 1  The GC is CPU intensive, employing heuristics to maximize procs usage between different operations modes (dedicated, idle and fractional GC) as it can be seen on the trace:\nThese heuristics force the CPU to work even under time frames that can worsen lock starvation, in the case of goroutines that end up giving running time to the scheduler.\nIn the following case, operating normally and outside a GC cycle, the goroutines are started and finished in a single time frame:\nUnder GC stress, the scheduler cannot grant enough time to the goroutine and the time frame partitioning can be seen for these goroutines, as it gives up running time and gets pre-empted multiple times by the scheduler until its finalization:\nAdd enough goroutines to the mix and each one will take an arbitrarily long amount of time and time frames to finish (as we saw with the starting issue).\nConclusion The neat part about this conclusion is that applies to pretty much every managed runtime: learn to profile your applications, and also learn to have a small and sane level of distrust of every third party line of code you run, because even something as innocuous as a log has side effects in regards of performance.\nOh, and log everything, but just not every single thing.\n","description":"A reminder that we should log, but maybe not just everything. And a performance rabbit hole","id":8,"section":"","tags":["Golang","Post-mortem"],"title":"Log everything, just not every single thing","uri":"https://luisgg.me/en/log_everything_just_not_every_single_thing/"},{"content":"This is a public post-mortem that was intended to be released on the engineering blog of MercadoLibre, but my departure (and the slowness of the editing) prevented that.\nThe event On December 28, 2018, the year\u0026rsquo;s last friday when our shipping calculator triggered an alert. This component is responsible of delivering the estimated delivery time along with the delivery options for a given article.\nFrom our team, in our NewRelic monitors we saw the cause was due to a strong degradation of the API response times. I guess it was time to leave aside the panettone leftovers and start diagnosing the issue.\nOur metrics pointed to a specific flow, SLA Coverage Brazil (carrier coverage by service level agreement). Under the hood one of its dependencies was the culprit: service coverage.\nThe diagnosis So, right now, our monitors showed that:\n In service coverage, the apdex and the execution/runtime metrics were degraded by a 100%, at the same time on the whole instance pool, during the window of just a couple of minutes This service failure triggers retries, adding an extra 300K requests per minute (RPM) and (not so) gracefully becoming a cascading failure\nIn the meanwhile, we were manually rebooting instances while adding new ones to the pool in an attempt to boost the service health. Additionally, we start observing replaced instances (that is, an instances that dies due to a process error and gets replaced by a new one).  So, from this point, the following events unfold:\n We identify test request starting before the event We locate the client and stop said traffic The instances slowly recover and the apdex goes green\nAmong the confusion this created (the traffic was just a couple dozen requests) we retraced our steps and found the real cause of the downtime, something we discarded on the beginning: the panics  After checking the service coverage logs, we had seen panics but quickly omitted them due to their frequency and the fact that they didn\u0026rsquo;t seem related to our particular problem\nWhat happened?  “\u0026hellip;panics do not propagate between goroutines, and must be handled independently.”\n When observing the panic stacktraces, we found the same pattern:\n The stack frames did not include any of our recovery mechanisms at all, so this wasn\u0026rsquo;t being handled The offending code came from a goroutine  As you can see in this issue, one design detail of golang that gets easily overlooked at first is that panics do not propagate between goroutines, and must be handled independently.\nWhen a panic happens inside a go application process, and does not recover it, it finalizes in an anomalous way (exit code ≠ 0).\nOur web application stack included a recovery middleware for all things happening in the entrant goroutine that gets covered by it, but any new goroutine must have its recovery mechanism defined explicitly.\nImplications (a.k.a panics for dummies) If a panic occurs on a gorountine without a recover(), the callee isn\u0026rsquo;t going to handle it, regardless of its recovery stack frame count, and the application will die.\nIt is a known issue and a non-negotiable design change for go 1.x (fingers crossed for go 2.x)\nCompare the following examples, this one has a propagated (unhandled panic):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { Go(test) time.Sleep(time.Second) fmt.Printf(\u0026#34;HELLO\u0026#34;) } func test() { panic(\u0026#34;PANIC\u0026#34;) } func Go(f func()) { defer func() { if r := recover(); r != nil { fmt.Printf(\u0026#34;RECOVERED ERROR %v\u0026#34;, r) } }() go f() }   And this one handles the panic. You may notice that this code is in need of some composability:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { Go(test) time.Sleep(time.Second) fmt.Printf(\u0026#34;HELLO\u0026#34;) } func test() { panic(\u0026#34;PANIC\u0026#34;) } func Go(f func()) { go func() { defer func() { if r := recover(); r != nil { fmt.Printf(\u0026#34;RECOVERED ERROR %v\u0026#34;, r) } }() f() }() }   Thinking some solutions A couple days after the incident, I gathered the team with some alternatives and let the votes decide which was the best one for our situation (checking and guarding a set of N apicalls across several repos).\nThere were 3 candidates:\nThe obvious and hard Not generating panics inside of the spawned goroutines. I\u0026rsquo;d wanted to patent that but I had the feeling Rob Pike already did that.\nThis is the ideal way but in the real world is not always possible. Most of time the are third-party dependencies or I/O driven packages which include panics as their error signaling mechanism. Getting rid of that is beyond us.\nThe idiomatic and ugly How about adding this lil' gut to the beginning every new goroutine we spawn in code?\n1 2 3 4 5  defer func() { if r := recover(); r != nil { fmt.Printf(\u0026#34;RECOVERED - %v\\r\\n\u0026#34;, r) } }()   It is going to work, with the disadvantage of breaking the DRY rule, becoming an extra point of failure for specialized handlers.\nRemember what I said about composability? we can mitigate it with a handler like the following, as long as f is argument of another function that launches the goroutine or we ourselves launch it:\n1 2 3 4 5 6 7 8 9 10  func Wrap(f func()) func() { return func() { defer func() { if r := recover(); r != nil { fmt.Printf(\u0026#34;RECOVERED - %v\\r\\n\u0026#34;, r) } }() f() } }   We\u0026rsquo;d use it the following way:\n1 2 3 4 5 6 7 8 9  func main() { go Wrap(test)() time.Sleep(time.Second) fmt.Println(\u0026#34;HELLO\u0026#34;) } func test() { panic(\u0026#34;PANIC\u0026#34;) } //…   This way, we centralize the panic handling. And last but not least, in case we want to return something from said handler, we communicate though channels:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { signal := make(chan bool) go WrapWithSignal(test, signal)() time.Sleep(time.Second) fmt.Println(\u0026#34;HELLO\u0026#34;) select { case sig := \u0026lt;-signal: if !sig { fmt.Println(\u0026#34;Could not finish test execution\u0026#34;) } else { fmt.Println(\u0026#34;Finished test execution\u0026#34;) } } } func test(signal chan bool) { panic(\u0026#34;PANIC\u0026#34;) signal \u0026lt;- true } func WrapWithSignal(f func(chan bool), signal chan bool) func() { return func() { defer func() { if r := recover(); r != nil { fmt.Printf(\u0026#34;RECOVERED - %v\\r\\n\u0026#34;, r) signal \u0026lt;- false } }() f(signal) } }   This simple pattern is idiomatic but can accumulate code smells quickly if the code has added complexity or use cases, so use it accordingly\nBonus track: Infrastructure In order to be resilient, a service must be able to respond quickly to failures. Throwing the entire instance and requesting a new one is far from an ideal solution (in this particular case, a simple reboot of the dying webserver with a watchdog script could\u0026rsquo;ve been orders of magnitude faster than the default workflow).\nBonus track II: Spotting unrecovered panics This was a dump of our middleware stack (notice the RecoveryWithWriter being the recovery middleware):\n1 2 3 4 5 6 7  vendor/[ommited]/gingonic/mlhandlers.Datadog.func1 vendor/github.com/newrelic/go-agent/_integrations/nrgin/v1.Middleware.func1 vendor/[ommited]/gingonic/mlhandlers.CommonAPiFilter.func1 vendor/[ommited]/gingonic/mlhandlers.RecoveryWithWriter.func1 context.AddContext [ommited]/src/api/context.AccessLog controllers.(*RouteCoverageController).ValidateGet-fm controllers.(*RouteCoverageController).Get-fm   And, the unhandled panic stacktrace on all of its glory:\n1 2 3 4 5 6 7 8 9  goroutine 141 [running]: [ommited]/src/api/services.(*CoverageService).GetCoverageServices(0xc4205ec1e0, 0x10e4e20, 0xc420073a70, 0xc420e8a41b, 0x3, 0xc420e8a41b, 0x3, 0x0, 0x0, 0x0, ...) /services/coverage_service.go:35 +0x39 [ommited]/src/api/services.(*CoverageLocationService).GetRouteCoverage.func1(0xc400000008, 0xca6f90) /services/coverage_locations.go:39 +0xe3 [ommited]/src/api/vendor/golang.org/x/sync/errgroup.(*Group).Go.func1(0xc42041e2c0, 0xc420426180) /vendor/golang.org/x/sync/errgroup/errgroup.go:57 +0x57 created by [ommited]/src/api/vendor/golang.org/x/sync/errgroup.(*Group).Go /vendor/golang.org/x/sync/errgroup/errgroup.go:54 +0x66 ```   The behavior is given away by 2 things:\n The code never passes through the recovery middleware The panic occurs inside of a goroutine launched by an errgroup.Group instance  ","description":"What happens when you forget a recover on a spawned goroutine?","id":9,"section":"","tags":["Golang","Post-mortem","Intermediate"],"title":"Forgetting that precious recover","uri":"https://luisgg.me/en/forgetting_that_precious_recover/"},{"content":"Note (December 2020): This is an article I wrote almost 4 years ago for my employer at that time, and now migrated to this space with some minor corrections.\nThe goal of this article is to give you a tour around advanced exceptions in .NET, in order to learn about their pitfalls and best practices when developing applications.\nException handling is a vital part of an application, no exceptions (pun intended).\nThe efficacy of this handling depends mostly on the language and platform of choice, so it’s important to know in detail their correct usage and behavior in order to spare our users and fellow developers from suffering when diagnosing issues in our code.\nIn this article we’re going to take a look at what C# and .NET do in regards of error handling.\nGlossary  CLR: Acronym for Common Language Runtime, is the .NET runtime, which is in charge of executing applications compiled in all of the .NET languages. Aside from the virtual machine and the Just-In-Time compiler it also has additional responsibilities like memory management, security, and so on. BCL: Acronym for Base Class Library, is the core library of the .NET framework. Besides operating directly with the CLR it exposes the primitive data types and essential functionality to build and run an application. Also known as mscorlib. FCL: Acronym for Framework Class Library, is what most of us know as “the framework” in .NET. Using the BCL as building blocks it exposes a great deal of namespaces with various features such as System.IO, System.Security, System.Text, among others. TPL: Acronym for Task Parallel Library, is the library that contains the functionality provided by the async keywords and API’s. Released with .NET version 4.5, along with C# 5. SEH: Acronym for Structured Exception Handling, is the native exception subsystem of Windows, it handles software and hardware exceptions on the operating system level MDA: Acronym for Managed Debugging Assistants. These are special debug extensions that provide information related to the CLR execution state to the Visual Studio debugger, which is exposed by internal helpers and assets. See the MSDN page for more details  Exceptions Revisited In .NET, and particularly in C#, exceptions are handled using try, catch and finally blocks. First of all, the try block will contain the code which is expected to throw an exception, and in second place we will have a catch block, that will specify an exception type and a code block that will be executed in the event that an exception matching the specified type is thrown inside the try block:\n1 2 3 4 5 6 7  Random rnd = new Random(); try { Console.WriteLine(1 / rnd.Next(-100, 101)); } catch (DivideByZeroException ex) { Debug.WriteLine(“A division by zero attempt has occurred”); }   It’s worth noting that an Exception type catch will handle all possible exceptions (this is not entirely true but we will stick with this assumption for now):\n1 2 3 4 5 6  try { //... } catch (Exception ex) { //General exception handler }   Another way of writing the general exception handler is the following:\n1 2 3  catch { //... }   A try block can have multiple catch blocks with different exception types associated, and those will be evaluated according to class hierarchy in a descending order:\n1 2 3 4 5 6  catch (DivideByZeroException ex) { //Code that handles division by zero exceptions } catch (Exception ex) { //Code that handles any exception that may occur }   NOTE: It’s important to keep in mind the evaluation order of exception types, since if our first catch handles an Exception type, all of the exception handlers below will be ignored. This is a common error that might be difficult to detect later in complex applications.\nAnd finally, we have the finally block (apologies for the redundancy), which is going to execute its contained code always, whether an exception has been thrown or not. So, a complete exception handler has the following form:\n1 2 3 4 5 6 7 8 9  try { //... } catch { //... } finally { //... }   It is worth noting (again) that the code inside a finally block isn’t included in the exception handling, so any exception thrown here won’t be caught, and it will bubble up the call stack until it’s managed.\nAs a last detail we can add that the throw keyword has a special meaning in the exception handling context, because it can also allow us to specify how to propagate the exception caught. Based on the previous example:\n1 2 3  try { Console.WriteLine(1 / rnd.Next(-100, 101)); }   We can propagate the exception in several ways inside the catch block:\n1 2 3 4 5 6 7 8 9 10 11  catch (DivideByZeroException ex) { //1 – Continue with the exception while preserving its stack trace, also called rethrow \tthrow; //2 – Throw the same type of exception but eliminating its original stack trace \tthrow ex; //3 – Throw a new exception, eliminating the original stack trace \tthrow new InvalidOperationException(); //4 - Throw a new exception, passing the current one as its InnerException \tthrow new InvalidOperationException(ex); }   A known limitation of async in C# is that the await keyword cannot be used inside catch and finally blocks. This was resolved in C# 6, and, since we’re talking about C# 6 we are going to mention a new feature added to the catch blocks, the exception filters. This new syntactic sugar lets us capture or propagate an exception according to the boolean value of an expression:\n1 2 3 4  try {…} catch (MyException e) when (filter(e)) {…}   One of the major advantages is that we can manage the exception flow without altering the stack trace. It can be also used as an interceptor, to add side effects like logging:\n1 2 3 4 5 6 7 8  private static bool Log(Exception e) { /* Exception logging */; return false; //This preserves the call stack } try {...} catch (Exception e) when (Log(e)) {...}   We still have to keep in mind the exception order we saw previously, to avoid issues like the following:\n1 2 3 4 5 6 7 8 9 10 11 12  catch (MyException ex) { // Will catch all instances of MyException } catch (MyException ex) when (filtro1(ex) \u0026amp;\u0026amp; filtro2(ex)) { // Unreachable } catch (MyException ex) when (filtro1(ex)) { // Unreachable }   Exceptions in .NET The table below shows a list with all base exceptions from the BCL (source: MSDN)\n Exception — Base Type: Object\n Base class for all exceptions.\n SystemException — Base Type: Exception\n Base class for all runtime-generated errors.\n IndexOutOfRangeException — Base Type: SystemException\n Thrown by the runtime only when an array is indexed improperly.\n NullReferenceException — Base Type: SystemException\n Thrown by the runtime only when a null object is referenced.\n AccessViolationException — Base Type: SystemException\n Thrown by the runtime only when invalid memory is accessed.\n InvalidOperationException — Base Type: SystemException\n Thrown by methods when in an invalid state.\n ArgumentException — Base Type: SystemException\n Base class for all argument exceptions.\n ArgumentNullException — Base Type: ArgumentException\n Thrown by methods that do not allow an argument to be null.\n ArgumentOutOfRangeException — Base Type: ArgumentException\n Thrown by methods that verify that arguments are in a given range.\n ExternalException — Base Type: SystemException\n Base class for exceptions that occur or are targeted at environments outside the runtime.\n COMException — Base Type: ExternalException\n Exception encapsulating COM HRESULT information.\n SEHException — Base Type: ExternalException\n Exception encapsulating Win32 structured exception handling information.\nWe can also include the type AggregateException that results from the execution of tasks and async code (TPL and Parallel LINQ). It’s a special exception type because it consolidates multiple exceptions in a single object, composing a tree of exceptions. For more information on usage techniques and use common use cases see the tasks and PLINQ articles on MSDN\nIn C# there are several keywords that include exception handling in their generated code:\n Using: Generates a try/finally, in which the Dispose method of the containing object is executed. Async/await: Async code is compiled to a state machine that manages method invocation transitions and delegation, and, among other things, it generates catch blocks that aggregate all exceptions thrown. Yield: In a similar fashion to async, coroutines (implemented in C# via the yield return statements) also generate a state machine with its respective exception handling code.  Exception dispatch One of the new features available since .NET 4.5, as part of the TPL release is the exception dispatch, facilitated by the class ExceptionDispatchInfo of the namespace System.Runtime.ExceptionServices.\nWith this class we can preserve an exception and delegate it to another instance, as long as we stay in the same AppDomain. Later on we can inspect the exception and throw it again (rethrow):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  ExceptionDispatchInfo exInfo = null; try { //... } catch (Exception ex) { exInfo = ExceptionDispatchInfo.Capture(ex); } //... if (exInfo != null) { exInfo.Throw(); }   Exceptional exceptions: Part I — CSE Up until now we had assumed that no matter what happens, all exceptions will be unconditionally caught by their associated catch blocks, and later on the finally block will be executed. Is this still true?\nThe answer is no, at least not always. By design, the CLR has some special exceptions types that cannot be captured, or at least they cannot be captured without forcing it via configuration.\nThe first kind are the Corrupted State Exceptions (CSE). They consist of a group of 8 native exceptions from Win32/SEH that, due to their nature, undergo the assumption that they’re not manageable since they imply that the program is in an invalid, inconsistent and unrecoverable state.\nThe native exceptions, along with their CLR counterparts, are the following:\n* EXCEPTION_ACCESS_VIOLATION — System.AccessViolationException\n* EXCEPTION_STACK_OVERFLOW — System.StackOverflowException\n* EXCEPTION_ILLEGAL_INSTRUCTION — SEHException\n* EXCEPTION_IN_PAGE_ERROR — SEHException\n* EXCEPTION_INVALID_DISPOSITION — SEHException\n* EXCEPTION_NONCONTINUABLE_EXCEPTION — SEHException\n* EXCEPTION_PRIV_INSTRUCTION — SEHException\n* STATUS_UNWIND_CONSOLIDATE — SEHException\nAt this instance, the exceptions are shown as “Unhandled exception” and are not observable from the local debug in Visual Studio. The only way to diagnose these in more detail is to use with its SOS extension and analyzing crash dumps, among others.\nThe reason behind them being ignored by managed code is that they are incoming exceptions from native code, which escape the application’s responsibility or, alternatively, we are dealing with a CLR bug, situation that is very unlikely but not impossible. It can also be a result of bugs in our code if we are using the unsafe feature of C# to write unmanaged code (with direct access to memory).\nIf we still wish to handle these exceptions, the CLR gives us the choice to catch some (not all) of them via the attribute HandleProcessCorruptedStateExceptions. This requires the method having an access level of SecurityCritical:\n1 2 3 4 5 6 7 8 9 10  [HandleProcessCorruptedStateExceptions] [SecurityCritical] public void MyMethod() { try { Marshall.StructureToPtr(1000, new IntPtr(1000), true); } catch {…} }   This can also be achived on a whole process by specifing the legacyCorruptedStateExceptionPolicy attribute as true, in our application configuration.\nNOTE: Due to compatibility reasons, The CSE’s will be removed in the upcoming versions of .NET Core. These will remain untouched in the desktop version, since it’s the original Windows version.\nThere are 2 remaining exceptions types that cannot be handled and indicate a premature and unrecoverable process termination: the StackOverflowExceptions (if originated within the CLR), and the CLR exceptions, which we’ll see next.\nExceptional exceptions: Part II — Exceptions in the CLR Diving deeper into the origin of exceptions, we stumble upon the CLR. The runtime is mostly build in C++, and has the task of dominating a complex scenario which is the handling of multiple exception types:\n Native Windows exceptions (SEH): The CLR unifies the handling of these exceptions with macros that simulate the VC++ compiler intrinsics (__try, __catch, etcecera). The SEH model is extremely intricate, making the unwind (we’ll see about that in the next section) a costly process in regards to stack frames and binary compatibility in comparison to linux, for example C++ exceptions: These exceptions can be thrown by the CLR code itself CLR exceptions: These are the exceptions delegated and exposed by the virtual machine to the application during execution  CoreCLR, the multiplatform and open source version of the CLR, has the even harder task of integrating compatibility with multiple platforms and architectures (Linux and Mac x64, among others in progress like x86 and ARM32/ARM64). To achieve the level of decoupling required, it relies on a layer called PAL (Platform Adaptation Layer).\nIn the event of a critical error inside the CLR it will throw an ExecutionEngineException (deprecated) and FatalExecutionEngineError (MDA). These errors commonly indicate heap corruptions in managed code.\nPerformance Implications To finish the exceptions tour, we’ll move on to a short but obligatory talk about the performance implications they have.\nException handling is very expensive compared to the execution of normal code. An exception incurs in the following actions and side effects:\n Cache errors and page faults in memory due to the execution flow (and context, consequently) changes. Unwind: this is the process in which the context belonging to the exception causing code in the try block is “cleaned”. This involves traversing all of the previous stack frames and the deallocation/disposing of the affected objects. All of this, additionally, may bring forward the next memory compaction by the garbage collector. Additional allocations for the diagnostic objects (StackTrace) “Cold” code accesses, which demand additional time for Just In Time compilation (JIT).\nIn normal situations there won’t be a noticeable performance impact, but the abuse of exceptions in situations with tight performance or scalability requirements can be troublesome.  To finish the article we hand out some tips that can be useful when designing and implementing efficient solutions:\n Avoid using exceptions as a control flow in our application. Use error codes or, even better, contemplate the errors and warning as part of your class design. Be measured with the exception throwing and catching: in web scenarios, for example, the exception handling can be centralized per request, or, in layered architectures, they can be handled per invocation across layers (dependency injection frameworks offer several tools for that end). Avoid having try blocks with excessive amounts of code, especially if the throwing code isn’t immediately related or dependent on it. This is because large code bodies (in both methods and blocks) can preemptively disable all runtime optimizations made by the Just In Time compiler (JIT). Currently (as of .NET Core 1.1) methods with try/catch blocks are not inlined (not even with MethodImplOptions.AggressiveInlining). Keep this in mind in order to prevent surprises  ","description":"A tour on advanced exceptions in .NET (circa 2016)","id":10,"section":"","tags":["C#","Generics"],"title":"Advanced exceptions in .NET","uri":"https://luisgg.me/en/advanced_exceptions_in_dotnet/"},{"content":"Note (December 2020): This is an article I wrote 4 years ago for my employer at that time, and now migrated to this space with some minor corrections.\nThe aim of this article is to give a brief and concise overview of the most advanced properties of generic types in C#.\nThis shall be useful for beginner developers and a practical review for the more experienced ones, in order to have a better understanding of how generics work and consequently being able to implement better designs.\nGlossary  JIT: Just-In-Time Compiler acronym, is the second .NET compilation instance, in which the intermediate code generated by the C# compiler is compiled (in real time and on demand) to assembly code, to then be executed by the CLR CLR: Acronym of the Common Language Runtime, is the execution time of the .NET platform, which is in charge of executing the applications compiled in all the languages ​​that comply to the Common Language Specification. In addition to the virtual machine and the Just-In-Time compiler it has additional responsibilities like memory handling, types, security, etc. BCL: Acronym of Base Class Library, is the core library of the .NET framework. In addition to operating directly with the CLR, it exposes the primitive types and essential functionality to be able to run an application. Also known as mscorlib FCL: Acronym of Framework Class Library, is what most developers known as “the framework” of .NET. Using the BCL as foundation this expose a large number of namespaces with varied and complete functionalities like System.IO, System.Security, System.Text, etc.\nGenerics are a powerful feature present in a large number of modern programming languages, and C# is no exception.\nIt brings with it several unknown and ignored concepts by a large part of the developer audience, even though we see and even use them daily, especially when consuming almost any type or generic interface of the .NET framework libraries, either BCL or FCL.\nBefore we dive deep into this, it is worth doing a brief review of generics (those who are already familiar with the concept can skip on to the next section).  Generics revisited What are generics? The short and simple answer is that it is a characteristic of typed programming languages that allows us to use types as parameters of other types. Or for the purists, better known as parametric polymorphism.\nUsing generics we can design interfaces and generic classes that have the same implementation, regardless of the type of parameter, increasing the flexibility of use in a safe way (type safety).\nIn this example we can see it in action:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  public class MyStack\u0026lt;T\u0026gt; { readonly int m_Size; readonly T[] m_Items; int m_StackPointer = 0; public MyStack(int size) { m_Size = size; m_Items = new T[m_Size]; } public T Pop() { m_StackPointer--; return m_Items[m_StackPointer]; } public void Push(T item) { m_Items[m_StackPointer] = item; m_StackPointer++; } }   For this generic class we can define an instance of MyStack(T) Where T can be any type we want: MyStack(string), MyStack(int), MyStack(object), MyStack(MyClass), MyStack(MyStruct)… The possibilities are infinite, although with the assurance that, under any kind of parameter with which we measure our MyStack (T), it will always behave in the same way we implement it, without the need for multiple overloads that accept parameters of other types.\n.NET specifics It is important to note that in .NET generics are reified, this means that parametric types are known at runtime through metadata — for more information see the following article.\nAlso, the JIT generates specialized code for each one, unlike Java (a close example) that deletes all parametric types at runtime.\nIn addition to the presence of accurate metadata for these types, the CLR has the following optimizations and techniques (among others) to mitigate potential penalties in performance:\n Boxing absence for primitive types thanks to specialization JIT Compilation of specialized code for each parametric type on demand, and dynamic load of these. When possible, the representation and stubs of compiled code are shared between different specializations Efficient support for specialization with native BCL / CLR types  Generic Constraints We can continue with the examples by adding a couple of classes to use as parametric types.\nIn this simple case, we want to have a list of contacts and we want to send them a greeting when adding them as a contact:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  public class ContactList\u0026lt;T\u0026gt; { public ContactList() { Contacts = new List\u0026lt;T\u0026gt;(); } public List\u0026lt;T\u0026gt; Contacts { get; set; } public void AddContact(T contact) { Contacts.Add(contact); Contacts.Greeting(\u0026#34;Hello! You have been added as a contact\u0026#34;); } } public class Employee : Person { public void Work() { Console.WriteLine(\u0026#34;I work really hard\u0026#34;); } } public class Person { public void Greeting(string message) { Console.WriteLine($\u0026#34;Greeting: {message}\u0026#34;); } public string Greeting() { return \u0026#34;Hello\u0026#34;; } }   The first thing we’ll notice is that this code doesn’t compile:\nT does not contain a definition for Greeting and no extension method Greeting accepting a first argument of type T could be found (are you missing a using directive or an assembly reference?)\nWe can solve it and make it compile by casting:\nWe take the following shortcut: “we elevate” the type of the contact by casting it to object, then we can apply the casting to the derived class Person without errors (since all classes derive from object).\nThe result is a tightly coupled code, with the addition that our generic class is no longer type safe, at the risk of throwing an InvalidCastException in case it’s an instance of a parametric type that doesn’t inherit from Person.\nWhat can we do to make our generic class only accept Person types or its derivatives, and at the same time be able to use the Person methods from within without resorting to arbitrary casting?\nLet’s change the definition of the class ContactList to the following:\n1 2 3 4  public class ContactList\u0026lt;T\u0026gt; where T : Person { ... }   In this way (with the keyword where) we are adding a generic constraint on the parameterized type T. The constraints are definitions that basically enforce compliance with certain requirements, in this case that is T being of Person type or a derived subclass. This is one of the many generic constraints that we can apply, and a generic class can have multiple constraints.\nThese are the constraints currently supported by C# (adapted from MSDN):\n Where T: struct\nThe argument type must be a value type. You can specify any value type except Nullable\nWhere T: class\nThe argument type must be a reference type; This also applies to any kind of class, interface, delegate, or array.\nWhere T: new()\nThe argument type must have a public constructor without parameters. When you use the new() constraint with other constraints, it must be specified last.\nWhere T : \nThe argument type must be the specified base class, or it must be derived from it.\nWhere T : \nThe argument type must be or implement the specified interface. Multiple interface restrictions can be specified. The constrained interface can also be generic.\nWhere T : U\nThe argument type provided for T must be or derive from the argument provided for U.\n Constraints are extremely useful when defining specialized generic interfaces on certain types that we have in mind to use, and also lets us target more specific types in our implementation. However, there are some limitations:\n You can’t use operators on parameterized type instances. This makes impossible the implementation of generic numerical algorithms with good performance, among other cases  Let’s look at the following example:\n1 2 3  private static void GenericCompare\u0026lt;T\u0026gt;(T o1, T o2) { if (o1 == o2) { } // Error }    Currently there is no constraint for numeric types, because they don’t have any interface in common and there is no support from the CLR to achieve this. This limitation is historical and is reflected in the APIs of the System.Math and similar ones, which are often plagued by overloaded methods. As we saw earlier, it is not possible to cast instances of a parameterized type without having to wrap the T instance in an object.  Covariance and countervariance Following what we’ve seen in the previous section we have a Person class and an Employee class, which inherits from Person. From this simple design, we can take advantage of the power of generics with the versatility of polymorphism … right?\nIf we try to compile the following code that at first seems reasonable…\n1 2  ContactList\u0026lt;Employee\u0026gt; people = null; people = new ContactList\u0026lt;Person\u0026gt;();   … We’ll get the following error:\nCannot implicitly convert type ‘ConsoleApplication1.ContactList’ to ‘ConsoleApplication1.ContactList’\nWhat went wrong? What happens is that generics allows us to use parameterized types in classes, but it doesn’t go as far as taking polymorphism into account. For this reason we can’t assign to a generic instance another with a derived parametric type, as we recently tried.\nOne of the first thoughts would be “How come .NET collections are so flexible and allow this?”\nThe answer lies in one of the final concepts of generics: variance, also known in C# for its 2 applications, covariance and countervariance\nWhat is the variance in C#? Variance is the interaction between types according to their subtyping relationship (inheritance). In a typing system or programming language that supports generic types such as C#, the concept can be extended to the parameterization of generic types and the relation between their parameterized types.\nIn simpler terms: it is the ability of polymorphism between parameter types of a generic class, in addition to what we were already assuming between common and generic types\nThe variance for parameterized generic types is present from the version 4.0 of C # (.NET 4.0 — CLR 4.0), while since version 3.0 (.NET 3.5 — CLR 2.0) it was already available, but only for delegates\nParametric types can be of 3 types:\n Invariant: The parametric type of the generic class can’t be changed, as we just saw in the previous example. In C#, parametric types are invariant by default Countervariant: The parameterized type can be converted to a derived class. Countervariant parameters can only be used at entrance points as the argument of a method. They are specified by the keyword in (eg: Action ) Covariant: The parameterized type can be converted to a base class. Countervariant parameters can only be used at exit points as the return type of a method. They are specified by the keyword out (eg Func \u0026lt;in T1, out T2\u0026gt;)  A limitation of co and countervariance is that it only applies to types for which there is reference conversion, so it is not possible to pass value types as parameter types if these are variants of the generic class. Another rather relative limitation is that it can only be applied to interfaces and delegates.\nAs a final point, it is worth clarifying that variance, strictly speaking, is not supported in classes and structs, but it’s supported in interfaces (on which we make assignments of class instances).\nReturning to the previous example, we can make our class countervariant in the following way:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  public interface IContactCollection\u0026lt;in T\u0026gt; { void AddContact(T contact); } public class ContactCollection\u0026lt;T\u0026gt; : IContactCollection\u0026lt;T\u0026gt; where T : Person { public List\u0026lt;T\u0026gt; contacts { get; set; } public ContactCollection() { contacts = new List\u0026lt;T\u0026gt;(); } public void AddContact(T contact) { contacts.Add(contact); contact.Greeting(\u0026#34;Hi! I added you as a contact\u0026#34;); } }   As you can see, we add a generic countervariant interface, which will implement our ContactList class, and keeping our generic constraint, allowing us to maintain type safety when instantiating and increasing compatibility when making assignments.\nThe following code is valid now:\n1 2  IContactCollection\u0026lt;Employee\u0026gt; people = null; people = new ContactCollection\u0026lt;Person\u0026gt;();   The benefits of designing classes in this way are instant to the time of consuming them, and for that reason the .NET team took the trouble of making co and countervariant much of the interfaces and delegates base of the BCL / FCL. To know:\nInterfaces/Delegates:\n Action delegates from namespace System, like Actiony Action\u0026lt;T1, T2\u0026gt; (T, T1, T2, … They are countervariant) Func delegates from namespace System, like, Funcand Func\u0026lt;T, TResult\u0026gt; (TResult is covariant; T, T1, T2, … They are countervariant) Predicate(T is countervariant) Comparison(T is countervariant) Converter\u0026lt;TInput, TOutput\u0026gt; (TInput is countervariant; TOutput is covariant)  Finally we can use these interfaces as an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  interface IInvariant\u0026lt;T\u0026gt; { // This interface can’t be explicitly cast  // Can be used for editable collections  IList\u0026lt;T\u0026gt; GetList { get; } // Can be used when T is parameter and return type.  T Metodo(T argument); } interface ICovariant\u0026lt;out T\u0026gt; { // This interface can be explicitly cast to other base types (upcasting)  // Can be used for readonly collections  IEnumerable\u0026lt;T\u0026gt; GetList { get; } // Can be used when T is a return type.  T Method(); } interface ICountervariant\u0026lt;in T\u0026gt; { // This interface can be explicitly cast to derived types (downcasting)  // Usually implies that T is used as argument type.  void Method(T argument); }   For more examples, you can go to the MSDN documentation, which contains a wide variety of use cases:\n Using Variance in interfaces for Generic Collections Using Variance for Func and Action Generic Delegates  ","description":"A tour on advanced generics in .NET (circa 2017)","id":11,"section":"","tags":["C#","Exceptions"],"title":"Advanced Generics in .NET","uri":"https://luisgg.me/en/advanced_generics_in_dotnet/"},{"content":"Note (December 2020): This is an article I wrote more than 3 years ago for my employer at that time, and now migrated to this space with some minor corrections. NET Core and CoreCLR Have grown exponentially in improvements and performance since the time of this post, but I also find interesting that most (if not all) of these concepts in software development still apply to this day, regardless of the platform\nWhen we talk about memory in the programming scene we refer to the computer memory in which our program will reside and also, eventually, occupy by itself when it executes its operations\nAnyone who had partaken into a programming course with an old agenda will know about this, because decades ago (when IT dinosaurs walked the earth) explicit and direct interaction with the memory was mandatory (think about C, C++, Pascal, ADA, and the list goes on…) and most of time it was even needed on a processor instruction level (Assembly).\n¿What happened with this ancestral and challenging way of programming then? Not much. It’s quite true that with the advent of web and enterprise applications the explicit memory management started to be seen as an unnecessary workload, an art reserved to certain niches like mission and security critical applications, systems with high scalability and workload requirements, videogames, operative systems and drivers, among others\nNowadays, the majority of programmers that started their careers with more modern languages like Java, C# or Visual Basic, are alien to these concepts, even having cases which never had to work with pointers or memory addresses.\nThe purpose of this article is to do a high level recap of memory management in modern languages (using .NET as a base, although may apply to many others) and see which solutions related to memory management and allocation we can find.\nGlossary  CLR: Acronym for Common Language Runtime, is the .NET runtime, which is in charge of executing applications compiled in all of the .NET languages. Aside from the virtual machine and the Just-In-Time compiler it also has additional responsibilities like memory management, security, and so on BCL: Acronym for Base Class Library, is the core library of the .NET framework. Besides operating directly with the CLR it exposes the primitive data types and essential functionality to build and run an application. Also known as mscorlib  Managed vs unmanaged memory Languages and development platforms of today rely in several techniques to manage the memory employed by its aplications in collaboration with the operative system to provide a great amount of abstraction, but it also lies along with the counterparts that don’t employ such benefits and do a manual memory management.\nAs a consequence of this differentiation in practice, two concepts come up:\n Managed memory: This type of memory is the one used by our traditional objects and classes, that don’t use resources or third party libraries. It’s not the only example although it’s the biggest one. In Java, these are known as POJO (Plain Old Java Object) and it’s analogous in .NET is POCO (Plain Old CLR Object). This memory is managed automatically by a subsystem specially designed for that purpose, the garbage collector (which we’ll see in the next section). We call managed languages to those that apply several abstraction measures to prevent or forbid the direct memory usage on behalf of the programmer Unmanaged or native memory: This group encloses the rest of the applications that do their own memory management, usually made on a lower level language that doesn’t make use of a garbage collector like C or C++, even though there are hybrid alternatives like Rust and D that give the option of having a garbage collector at runtime. Resources deserve a special mention, they are another group that encompass hardware related code and pointers, external systems and the operative system, with concepts like handles, connections, I/O and OS primitives (synchronization, security, etcetera)  Following the previous reasoning, the availability of unmanaged elements in managed scenarios is due to the fact that some managed objects act as wrappers of resources to employ, and handle accordingly the memory usage and disposal of these. Examples can be found in the APIs System.IO or System.Runtime.InteropServices\nThe communication process between libraries, in the context between managed and unmanaged is called interop (as in interoperability), and the process of transition between native memory to managed objects is called marshalling. We can think of it as a serialization analogy but between native memory and our application objects. In .NET, the interop features are provided by the BCL, via the InteropServices namespace along with the static extern C# keywords, to define the signatures of the external methods to call. It’s also known as P/Invoke\nThe garbage collector: an unexpected ally Now that we had a humble introduction to memory management, it’s the turn of one of its main protagonists: the garbage collection (we will refer to it as GC). It consists of a subsystem that usually does the cleanup of memory that is no longer used by our application (and in some cases, like .NET, also handles the memory allocation).\nThe design and implementation of garbage collectors is an extremely complex topic that span entire books, and the implementation details of the .NET GC alone deserves a separate article, so we will just summarize the main features, objectives and responsibilities that takes on its own:\n Allocation and defragmentation of memory, being the main component of the CLR regarding memory management along with the execution engine It comes in blocking (single thread) and parallel (multi thread­) versions. Both can have very different effects in the performance of the same program It’s of the mark \u0026amp; sweep and generational types, which means that instead of following all of the objects’ life cycles it only marks them in order to collect them once they are dereferenced, and are divided into generations according to their age. Collections are executed one generation at a time  The main objectives of a GC are:\n Efficiency: Do quick and infrequent collections Efficacy: Collect most (if not all) of the dereferenced memory Correctness and consistency: False positives are unacceptable. Called GC holes, are instances of objects that are wrongly marked as dereferenced. It also has to mark, eventually, all objects that are dereferenced  To achieve these objectives, it relies on a lot of heuristics and optimizations along with the CLR.\nIt’s also important to know that it’s non deterministic. This means that the developer cannot control the behavior of the GC at will, it can only receive hints to its heuristics.\nManaged languages contain a given implementation of a GC, but there cases like Java that have several implementations, some of them commercial and developed by third party vendors. For more information about the .NET garbage collector I recommend Maoni Stephens’ blog, she is the development lead of the .NET GCs.\nMemory leaks and other mythical beasts As we saw previously, the GC does a great job when it comes to manage the memory that we, the developers, use, although it can’t manage all scenarios, so it depends on our help and good practices to keep doing its job properly. This is especially true in large applications that make extensive use of resources and have large object trees.\nThe first issues that arise due to a poor memory management are memory leaks. These situations occur when there are idle objects in memory that are no longer needed, but are still referenced by other objects, so they can’t be claimed by the GC. This also happens when we do an inappropriate cleanup of unmanaged memory, because it’s out of bounds for the GC (unless it’s part of a managed wrapped being collected).\nA classic example of a memory leak caused by the programmer’s negligence is the following:\n1 2 3 4 5  static event EventHandler Leaker; for(int i = 0 ; i \u0026lt; 100000 ; i++) { Leaker += delegate {}; }   In this scenario, a massive event handler registration is being made on a static field, which implies that these objects will never be claimed because these never get out of a scope and get dereferenced. The events hold the subscribers’ references, and if not controlled, they will reside in memory forever.\nAnother variant is what happens when object life cycles are extended (even if we know that they are going to be dereferenced and collected). This can induce temporary memory leaks due to object recollection retardation, and in some cases it can cause exceptions due to thread safety or resource consumption issues.\nFinally, we have the most unusual but dangerous variant, the handle leaks. Leaking handles is more risky than leaking memory due to a simple reason: the CLR is extremely good at managing memory, but handles are a native OS resource, and the CLR has no say over them. Handle starvation and misuse can cause widespread problems like device access issues, OS instability, memory corruption and crashes.\nMemory leaks: a personal experience A personal horror story in .NET goes back to the abuse of coroutines (yield return) and extended life cycles in parallel code. The original code I had to maintain did the following:\n A ForAll expression was run over a collection of objects that retrieved data from a database These objects consumed a singleton DataReader via a coroutine, using connected ADO The code was parallel, so multiple calls were being made towards the DataReader at the same time To prevent invalid access exceptions (since DataReader is not safe) the database call section was blocking, which “assured” that the code was thread safe  Trusting the original code was a mistake, since the autor and I forgot a detail that seemed minor but took me hours and hours of debugging: the coroutines capture the objects being exposed via the IEnumerator returned by it, which extends the life cycle of the object. This delayed the DataReader object and resource finalization, thus resulting in unpredictable DataReader access exceptions\nThe moral of the story is: trust no one, not even your language of choice’s syntactic sugar\nManaged memory in .NET In .NET, all objects are allocated in memory (be it stack, heap or the large object heap — LOH) in 3 ways: from the JIT compiler, from the compiled code o directly to the LOH (for more info check this). Direct allocation of data into register is done later, during the parameter passing and optimizations done during JIT compilation.\nGiven that, we can infer that all objects will be eventually collected by the GC, assuming that the developer was responsible and didn’t cause any memory leaks. Later on, the objects are directly collected and the memory freed, although there are exceptions. In C# we have finalizers, which is an analog of C++ class destructors. Instances of class that implement them will be taken by the GC upon collection and put into a finalization queue, separated from traditional objects, that will dispatch objects as soon as their internal references are collected, and after that, it will trigger the finalizer of said instance. This stage is non deterministic and we cannot control the exact moment nor order of execution.\nWe have 2 cases that enter in this category:\n IDisposable objects: The IDisposable interface is special because, depending on the implementation, it will have different behaviors at GC time. A complete implementation of the pattern with unmanaged resources that use finalizers will incur in the behavior we explained above CriticalFinalizerAttribute: This attribute signals the CLR to put the instance in the last place of the finalization queue, and ensures that the finalizer will be run, regardless of the state of the execution engine. It’s used in low level classes at the BCL level like SafeHandle, a managed wrapper for handles that will dispose the handle even if the program finishes its execution in an anomalous fashion  Unmanaged and native memory in C# The presence of unmanaged memory in .NET applications is reduced to these scenarios:\n Interop: This is the most usual use case, be it for communications with third party libraries or WinAPI. Concepts like marshalling, calling conventions and OS handles are important here, and all classes that implement these methods and/or have access to handles should be recycled correctly via the IDisposable pattern, and consumed via the using keyword Resources: They were mentioned in the point above but it’s worth noting that resource usage is a source of unmanaged memory. I/O, direct memory, GPU or connections to servers, these generate memory which is not handled by the CLR, but counts as memory in terms of usage by our program. Memory leaks of this type may be harder to detect than their managed counterparts Unsafe code: A relatively unknown feature of C# is the possibility to define methods and code areas with the unsafe keyword, which gives the developer the capabilities of pointer operations and object pinning, so it won’t be moved by the GC upon memory compaction, thus allowing to operate with memory addresses and offsets. It’s rarely used, but valuable in extreme microoptimization scenarios Unmanaged APIs: There are methods that allow to allocate unmaged memory in an explicit way, and return a handle (IntPtr class in the BCL) to manipulate said memory. If misused, this is a really easy way to leak memory and induce exceptions of type AccessViolationException, which are not manageable as regular exceptions  The following example shows how to save and read number in unmanaged memory via the Marshall API\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  namespace MemDemo { using System; using System.Runtime.InteropServices; class AllocDemo { static void Main() { //Calculate the int size since it\u0026#39;s architecture dependent  const int int32Size = sizeof(int); //Allocate the memory for 10 numbers  IntPtr hglobal = Marshal.AllocHGlobal(int32Size * 10); //Write the numbers into memory using the count as offset  for (var i = 0; i \u0026lt; 10; i++) { Marshal.WriteInt32(hglobal, int32Size * i, i); } Console.WriteLine($\u0026#34;{int32Size} bytes of memory allocated and initialized.\u0026#34;); //Read the numbers from memory using the count as offset  string values = \u0026#34;{\u0026#34;; for (var i = 0; i \u0026lt; 10; i++) { values += Marshal.ReadInt32(hglobal, int32Size * i) + \u0026#34; \u0026#34;; } values += \u0026#34;}\u0026#34;; Console.WriteLine($\u0026#34;{int32Size} bytes of memory read. Values are {values}\u0026#34;); //Free the used memory, otherwise we\u0026#39;d causing a memory leak  Marshal.FreeHGlobal(hglobal); Console.WriteLine(\u0026#34;Memory deallocated successfully\u0026#34;); Console.ReadLine(); } } }   And in the following example we’ll do a special hello world, using the kernel native method to output the message. This will be printed to the application debugger output (in Visual Studio it can be seen in the Output window). This operates entirely out of the heap space, as it interoperates with the Windows Kernel API:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  namespace MemDemo { using System; using System.Runtime.InteropServices; class WinapiDemo { [DllImport(\u0026#34;kernel32.dll\u0026#34;)] static extern void OutputDebugString(string lpOutputString); static void Main() { OutputDebugString(\u0026#34;Hello world!\u0026#34;); Console.ReadLine(); } } }   ","description":"A post about one of the most overlooked topics by modern developers: memory","id":12,"section":"","tags":["C#"],"title":"Memory: A Forgotten Topic","uri":"https://luisgg.me/en/memory_a_forgotten_topic/"},{"content":"For my work experience instead of this page you may want to check this instead: resume (sha256sum checksum)\nI\u0026rsquo;m a software developer, occasional software architect, and jack of some trades based in Buenos Aires, Argentina. I can say that my job and hobby are software development and technology in general. I\u0026rsquo;ve worked at small scale in software factories, at large scale in an e-commerce giant and currently I\u0026rsquo;m helping redefine the technology stack of the local chapter of a multinational bank. I am a student of electronics engineering (formerly software engineering, but after getting that career on board, I guess I want to study something more personally fulfilling and in my list of TODOs).\nI started as a full stack .NET developer (anyone remembers the simpler days with MVVM frameworks and JQuery?) but my main domain today lies in wrangling with backend systems and trying to keep them resilient and distributed, among other things.\nMy main tool of trade is golang as of today and I may rant about a lot of technologies and languages, but I certainly don\u0026rsquo;t marry with any technology in particular. I still believe that certain tools are better suited for some scenarios than others.\nI also like doing my job the best way I can (given the circumstances). While it is true that it rarely is a best (obvious) path to solve a complex scenario it is also true that the plethora of terrible solutions are obvious, and I (and my team mates, luckily) do care for the execution of this trade-off as best as possible.\nI know there may look like there is some helpless vanity in this endeavor, like trying to take the best selfie on an impending snowstorm:\nBut in the end of the day (end may vary due to debugging circumstances) it is nice to see something durable that can withstand anything (anything may also vary due to QA circumstances) and last for long, long time:\nPS: I also like doing photography, but since this is something I do exclusively on vacations I consider it a no-op:\n","description":"About the blog author, Luis Gabriel Gomez","id":13,"section":"","tags":null,"title":"About me","uri":"https://luisgg.me/en/about/"}]